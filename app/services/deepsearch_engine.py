from typing import Any, Dict, List, Optional, TypedDict, Callable, TypeVar
import asyncio
import inspect
import httpx

from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_openai import ChatOpenAI
from typing_extensions import Annotated
from langgraph.graph import add_messages
import operator
import logging

from app.core.config import settings

T = TypeVar('T')
from .deepsearch_prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
    content_quality_instructions,
    fact_verification_instructions,
    relevance_assessment_instructions,
    summary_optimization_instructions,
    research_plan_instructions,
)
from .deepsearch_utils import (
    get_citations_from_bocha,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
    format_bocha_search_results,
)
from .web_scraper import scrape_webpages, clean_and_truncate
from .deepsearch_types import (
    SearchQueryList, 
    Reflection,
    ContentQualityAssessment,
    FactVerification,
    RelevanceAssessment,
    SummaryOptimization,
    ResearchPlan,
)


logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šè·Ÿè¸ªæ˜¯å¦å·²é™çº§åˆ° Qwen3Max
_gemini_degraded = False

# å…¨å±€å–æ¶ˆçŠ¶æ€ç®¡ç†ï¼ˆä½¿ç”¨ asyncio.Event æ›´å®‰å…¨åœ°é¿å…ç«æ€ï¼‰
_connection_cancellations: Dict[str, asyncio.Event] = {}
_cancellations_lock = asyncio.Lock()


def reset_degradation_status():
    """é‡ç½®é™çº§çŠ¶æ€ï¼ˆç”¨äºæµ‹è¯•æˆ–æ–°è¯·æ±‚å¼€å§‹æ—¶ï¼‰."""
    global _gemini_degraded
    _gemini_degraded = False
    logger.info("ã€é™çº§çŠ¶æ€ã€‘å·²é‡ç½®ï¼Œå°†é‡æ–°å°è¯• Gemini")


async def set_connection_cancelled(connection_id: str):
    """æ ‡è®°è¿æ¥ä¸ºå·²å–æ¶ˆ."""
    global _connection_cancellations
    async with _cancellations_lock:
        event = _connection_cancellations.get(connection_id)
        if event is None:
            event = asyncio.Event()
            _connection_cancellations[connection_id] = event
        event.set()
    logger.info(f"ã€å–æ¶ˆçŠ¶æ€ã€‘è¿æ¥ {connection_id} å·²è¢«æ ‡è®°ä¸ºå–æ¶ˆ")


def is_connection_cancelled(connection_id: str) -> bool:
    """æ£€æŸ¥è¿æ¥æ˜¯å¦å·²è¢«å–æ¶ˆ."""
    global _connection_cancellations
    event = _connection_cancellations.get(connection_id)
    return event.is_set() if event else False


async def cleanup_connection_cancellation(connection_id: str):
    """æ¸…ç†è¿æ¥çš„å–æ¶ˆçŠ¶æ€."""
    global _connection_cancellations
    async with _cancellations_lock:
        if connection_id in _connection_cancellations:
            del _connection_cancellations[connection_id]
    logger.info(f"ã€å–æ¶ˆçŠ¶æ€ã€‘è¿æ¥ {connection_id} çš„å–æ¶ˆçŠ¶æ€å·²æ¸…ç†")


def check_cancellation_and_raise(connection_id: Optional[str] = None):
    """æ£€æŸ¥å–æ¶ˆçŠ¶æ€ï¼Œå¦‚æœè¢«å–æ¶ˆåˆ™æŠ›å‡ºCancelledError."""
    if connection_id and is_connection_cancelled(connection_id):
        logger.info(f"ã€å–æ¶ˆæ£€æŸ¥ã€‘è¿æ¥ {connection_id} å·²è¢«å–æ¶ˆï¼Œåœæ­¢æ‰§è¡Œ")
        raise asyncio.CancelledError(f"è¿æ¥ {connection_id} å·²è¢«å–æ¶ˆ")


def get_qwen_base_url() -> str:
    """è·å– Qwen3Max API base URL."""
    base_url = settings.DASHSCOPE_BASE_URL
    if not base_url:
        raise ValueError("DASHSCOPE_BASE_URL is not set")
    # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
    base_url = base_url.rstrip('/')
    # ç¡®ä¿ä»¥ /v1 ç»“å°¾
    if not base_url.endswith('/v1'):
        base_url = f"{base_url}/v1"
    logger.debug(f"Qwen3Max API base URL: {base_url}")
    return base_url


def get_gemini_base_url() -> str:
    """è·å– Gemini API base URLï¼Œç¡®ä¿ä»¥ /v1 ç»“å°¾"""
    base_url = settings.GEMINI_API_URL
    if not base_url:
        raise ValueError("GEMINI_API_URL is not set")
    # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
    base_url = base_url.rstrip('/')
    # ç¡®ä¿ä»¥ /v1 ç»“å°¾
    if not base_url.endswith('/v1'):
        base_url = f"{base_url}/v1"
    logger.debug(f"Gemini API base URL: {base_url}")
    return base_url


if not settings.GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY is not set")


def create_llm_with_fallback(
    model: str,
    temperature: float,
    use_gemini: bool = True,
    max_retries: int = 2
) -> ChatOpenAI:
    """
    åˆ›å»º LLM å®ä¾‹ï¼Œæ”¯æŒ Gemini å’Œ Qwen3Max åˆ‡æ¢.
    
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        use_gemini: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ Geminiï¼ˆTrueï¼‰æˆ– Qwen3Maxï¼ˆFalseï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        ChatOpenAI: LLM å®ä¾‹
    """
    if use_gemini:
        base_url = get_gemini_base_url()
        api_key = settings.GEMINI_API_KEY
        logger.debug(f"åˆ›å»º Gemini LLM å®ä¾‹: {model}")
    else:
        base_url = get_qwen_base_url()
        api_key = settings.DASHSCOPE_API_KEY
        if not api_key:
            raise ValueError("DASHSCOPE_API_KEY is not set")
        logger.debug(f"åˆ›å»º Qwen3Max LLM å®ä¾‹: {model}")
    
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        max_retries=max_retries,
        api_key=api_key,
        base_url=base_url,
        timeout=settings.API_TIMEOUT,
    )


async def invoke_llm_with_fallback(
    invoke_func: Callable[[ChatOpenAI], T],
    node_name: str,
    gemini_model: str,
    temperature: float = 0.5,
    qwen_model: str = None,
    structured_output_type: Any = None,
    connection_id: Optional[str] = None,
    **llm_kwargs
) -> T:
    """
    å¼‚æ­¥è°ƒç”¨ LLMï¼Œæ”¯æŒ Gemini å¤±è´¥åè‡ªåŠ¨åˆ‡æ¢åˆ° Qwen3Max.
    
    å¦‚æœå·²ç»é™çº§åˆ° Qwen3Maxï¼Œç›´æ¥ä½¿ç”¨ Qwen3Maxï¼Œä¸å†å°è¯• Geminiã€‚
    å¦åˆ™å…ˆå°è¯•ä½¿ç”¨ Geminiï¼ˆé‡è¯•2æ¬¡ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™åˆ‡æ¢åˆ° Qwen3Maxï¼Œå¹¶è®¾ç½®å…¨å±€é™çº§æ ‡å¿—ã€‚
    
    Args:
        invoke_func: å¼‚æ­¥è°ƒç”¨å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªå‚æ•°ï¼ˆllm å®ä¾‹ï¼‰å¹¶è¿”å›ç»“æœï¼ˆæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥å‡½æ•°ï¼‰
        node_name: èŠ‚ç‚¹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        gemini_model: Gemini æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        qwen_model: Qwen3Max æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ DASHSCOPE_CHAT_MODELï¼‰
        structured_output_type: ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼ˆå¦‚æœæä¾›ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ with_structured_outputï¼‰
        connection_id: è¿æ¥IDï¼Œç”¨äºå–æ¶ˆæ£€æŸ¥
        **llm_kwargs: ä¼ é€’ç»™ ChatOpenAI çš„å…¶ä»–å‚æ•°
        
    Returns:
        è°ƒç”¨ç»“æœ
        
    Raises:
        Exception: å¦‚æœä¸¤ç§æ¨¡å‹éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
        asyncio.CancelledError: å¦‚æœè¿æ¥è¢«å–æ¶ˆ
    """
    global _gemini_degraded
    
    # åœ¨æ¯æ¬¡é‡è¯•å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    if qwen_model is None:
        qwen_model = settings.DASHSCOPE_CHAT_MODEL
    
    # å¦‚æœå·²ç»é™çº§ï¼Œç›´æ¥ä½¿ç”¨ Qwen3Max
    if _gemini_degraded:
        logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘å·²é™çº§ï¼Œç›´æ¥ä½¿ç”¨ Qwen3Max ({qwen_model})...")
        try:
            # å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            check_cancellation_and_raise(connection_id)
            
            llm = create_llm_with_fallback(
                model=qwen_model,
                temperature=temperature,
                use_gemini=False,
                max_retries=2
            )
            
            # å¦‚æœæŒ‡å®šäº†ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼Œä½¿ç”¨ with_structured_output
            if structured_output_type is not None:
                llm = llm.with_structured_output(structured_output_type)
            
            # è°ƒç”¨å¹¶æ ¹æ®å¯ç­‰å¾…æ€§è¿›è¡Œå¤„ç†ï¼ˆå…¼å®¹åŒæ­¥å‡½æ•°è¿”å›åç¨‹çš„æƒ…å†µï¼‰
            _maybe = invoke_func(llm)
            result = await _maybe if inspect.isawaitable(_maybe) else _maybe
            
            # æˆåŠŸè¿”å›åç«‹åˆ»å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆï¼Œé¿å…æ–­å¼€åç»§ç»­è¾“å‡º
            check_cancellation_and_raise(connection_id)
            logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Qwen3Max è°ƒç”¨æˆåŠŸ")
            return result
        except Exception as e:
            logger.error(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Qwen3Max è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            raise e
    
    # å°è¯•ä½¿ç”¨ Geminiï¼ˆé‡è¯•1æ¬¡ï¼‰
    last_error = None
    for attempt in range(2):  # ç¬¬ä¸€æ¬¡ + 1æ¬¡é‡è¯• = æ€»å…±2æ¬¡å°è¯•
        try:
            # æ¯æ¬¡é‡è¯•å‰éƒ½æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            check_cancellation_and_raise(connection_id)
            
            base_url = get_gemini_base_url()
            api_key = settings.GEMINI_API_KEY
            logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘å°è¯•ä½¿ç”¨ Gemini ({gemini_model})...")
            
            # æ¯æ¬¡é‡è¯•éƒ½é‡æ–°åˆ›å»ºLLMå®ä¾‹
            llm = ChatOpenAI(
                model=gemini_model,
                temperature=temperature,
                api_key=api_key,
                base_url=base_url,
                timeout=settings.API_TIMEOUT,
                max_retries=1,  # å†…éƒ¨ä¸å†é‡è¯•ï¼Œç”±å¤–å±‚æ§åˆ¶
                **llm_kwargs
            )
            
            # å¦‚æœæŒ‡å®šäº†ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼Œä½¿ç”¨ with_structured_output
            if structured_output_type is not None:
                llm = llm.with_structured_output(structured_output_type)
            
            logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini è°ƒç”¨å¼€å§‹...")
            
            # è°ƒç”¨å¹¶æ ¹æ®å¯ç­‰å¾…æ€§è¿›è¡Œå¤„ç†ï¼ˆå…¼å®¹åŒæ­¥å‡½æ•°è¿”å›åç¨‹çš„æƒ…å†µï¼‰
            _maybe = invoke_func(llm)
            result = await _maybe if inspect.isawaitable(_maybe) else _maybe
            
            # æˆåŠŸè¿”å›åç«‹åˆ»å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆï¼Œé¿å…æ–­å¼€åç»§ç»­è¾“å‡º
            check_cancellation_and_raise(connection_id)
            logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini è°ƒç”¨æˆåŠŸ")
            return result
            
        except Exception as e:
            last_error = e
            logger.warning(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/2): {str(e)}", exc_info=False)
            
            if attempt == 0:
                logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini ç¬¬ 1 æ¬¡é‡è¯•...")
                # é‡è¯•å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                check_cancellation_and_raise(connection_id)
            else:
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œåˆ‡æ¢åˆ° Qwen3Max
                logger.warning(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini é‡è¯•{attempt}æ¬¡åä»å¤±è´¥ï¼Œåˆ‡æ¢åˆ° Qwen3Max ({qwen_model})...")
                
                # åˆ‡æ¢å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                check_cancellation_and_raise(connection_id)
                
                # è®¾ç½®å…¨å±€é™çº§æ ‡å¿—
                _gemini_degraded = True
                logger.warning(f"ã€é™çº§çŠ¶æ€ã€‘å·²è®¾ç½®é™çº§æ ‡å¿—ï¼Œåç»­æ‰€æœ‰è°ƒç”¨å°†ç›´æ¥ä½¿ç”¨ Qwen3Max")
                
                # é‡æ–°é€’å½’è°ƒç”¨ï¼Œè¿™æ¬¡ç›´æ¥ä½¿ç”¨ Qwen3Max
                return await invoke_llm_with_fallback(
                    invoke_func=invoke_func,
                    node_name=node_name,
                    gemini_model=gemini_model,
                    temperature=temperature,
                    qwen_model=qwen_model,
                    structured_output_type=structured_output_type,
                    connection_id=connection_id,
                    **llm_kwargs
                )
    
    # å¦‚æœåˆ°è¿™é‡Œï¼Œè¯´æ˜æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
    logger.error(f"ã€èŠ‚ç‚¹: {node_name}ã€‘æ‰€æœ‰æ¨¡å‹è°ƒç”¨éƒ½å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")
    raise last_error


class OverallState(TypedDict, total=False):
    messages: Annotated[List, add_messages]
    research_plan: Optional[ResearchPlan]
    search_query: Annotated[List, operator.add]
    new_search_query: List[str]  # æœ¬è½®æ–°ç”Ÿæˆçš„æŸ¥è¯¢ï¼ˆä¸ç´¯åŠ ï¼‰
    web_research_result: Annotated[List, operator.add]
    sources_gathered: Annotated[List, operator.add]
    all_sources_gathered: Annotated[List, operator.add]  # æ‰€æœ‰æœç´¢åˆ°çš„èµ„æºï¼ˆåŒ…æ‹¬æœªè¢«å¼•ç”¨çš„ï¼‰
    initial_search_query_count: int
    max_research_loops: int
    research_loop_count: int
    reasoning_model: str
    unanswered_questions: List[str]  # æœªå›ç­”çš„ç ”ç©¶é—®é¢˜åˆ—è¡¨ï¼ˆæ¯è½®æ›¿æ¢ï¼Œä¸ç´¯åŠ ï¼‰
    # è´¨é‡å¢å¼ºç›¸å…³å­—æ®µ
    content_quality: Dict[str, Any]
    fact_verification: Dict[str, Any]
    relevance_assessment: Dict[str, Any]
    summary_optimization: Dict[str, Any]
    verification_report: str
    final_confidence_score: float


class ReflectionState(TypedDict):
    is_sufficient: bool
    knowledge_gap: str
    unanswered_questions: List[str]  # æ¯è½®æ›¿æ¢ï¼Œä¸ç´¯åŠ 
    research_loop_count: int
    number_of_ran_queries: int
    max_research_loops: int  # æ·»åŠ æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•°å­—æ®µ


class Query(TypedDict):
    query: str
    rationale: str


class QueryGenerationState(TypedDict):
    search_query: List[str]  # å®é™…ä¸Šæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸æ˜¯ Query å¯¹è±¡åˆ—è¡¨
    new_search_query: List[str]  # æœ¬è½®æ–°ç”Ÿæˆçš„æŸ¥è¯¢ï¼ˆä¸ç´¯åŠ ï¼‰


class WebSearchState(TypedDict):
    search_query: str
    id: str


async def generate_research_plan(state: OverallState, config: RunnableConfig) -> OverallState:
    """
    ç”Ÿæˆç ”ç©¶æ–¹æ¡ˆèŠ‚ç‚¹ã€‚
    """
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥ï¼Œå®‰å…¨å¤„ç†configå¯¹è±¡
    connection_id = None
    if config:
        # å¤„ç†configå¯èƒ½æ˜¯RunnableConfigå¯¹è±¡æˆ–å­—å…¸çš„æƒ…å†µ
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    logger.info("ã€èŠ‚ç‚¹: generate_research_planã€‘å¼€å§‹ç”Ÿæˆç ”ç©¶æ–¹æ¡ˆ...")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    research_topic = get_research_topic(state["messages"])
    logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç ”ç©¶ä¸»é¢˜: {research_topic[:200]}...")
    
    formatted_prompt = research_plan_instructions.format(
        research_topic=research_topic
    )
    
    logger.info("ã€èŠ‚ç‚¹: generate_research_planã€‘è°ƒç”¨ LLM ç”Ÿæˆæ–¹æ¡ˆ...")
    try:
        plan = await invoke_llm_with_fallback(
            invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
            node_name="generate_research_plan",
            gemini_model=reasoning_model,
            temperature=0.3,  # ä»0.5é™ä½åˆ°0.3ï¼Œè®©è¾“å‡ºæ›´ä¸¥è°¨è¯¦ç»†
            structured_output_type=ResearchPlan,
            connection_id=connection_id
        )
        # è¿”å›åå†æ¬¡æ£€æŸ¥å–æ¶ˆï¼Œé¿å…åç»­æ—¥å¿—ç»§ç»­è¾“å‡º
        check_cancellation_and_raise(connection_id)
        logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç ”ç©¶æ–¹æ¡ˆç”Ÿæˆå®Œæ¯•ï¼ŒåŒ…å« {len(plan.sub_topics)} ä¸ªå­ä¸»é¢˜")
        logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç ”ç©¶é—®é¢˜æ€»æ•°: {len(plan.research_questions)}")
        for idx, sub_topic in enumerate(plan.sub_topics, 1):
            logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘  å­ä¸»é¢˜ {idx}: {sub_topic}")
        return {"research_plan": plan}
    except asyncio.CancelledError:
        # ä¸»åŠ¨å–æ¶ˆæ—¶ä¸è®°å½•é”™è¯¯ï¼Œå‘ä¸ŠæŠ›å‡ºä»¥å°½å¿«åœæ­¢å›¾æ‰§è¡Œ
        logger.info("ã€èŠ‚ç‚¹: generate_research_planã€‘æ£€æµ‹åˆ°å–æ¶ˆï¼Œç»ˆæ­¢èŠ‚ç‚¹æ‰§è¡Œ")
        raise
    except Exception as e:
        logger.error(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç”Ÿæˆæ–¹æ¡ˆå¤±è´¥: {e}", exc_info=True)
        return {"research_plan": None}


async def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    logger.info("ã€èŠ‚ç‚¹: generate_queryã€‘å¼€å§‹ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
    
    # åˆ¤æ–­è¿è¡Œæ¨¡å¼ï¼šé¦–æ¬¡è¿è¡Œ vs é’ˆå¯¹æ€§è¿è¡Œ
    unanswered_questions = state.get("unanswered_questions", [])
    is_targeted_mode = len(unanswered_questions) > 0
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    research_topic = get_research_topic(state["messages"])
    research_plan = state.get("research_plan")
    
    # å°†æ–¹æ¡ˆæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    plan_str = "æ— ç‰¹å®šæ–¹æ¡ˆï¼Œè¯·ç›´æ¥åˆ†æç ”ç©¶ä¸»é¢˜ã€‚"
    if research_plan and research_plan.sub_topics:
        plan_str = f"ä¸»é¢˜: {research_plan.research_topic}\n\nå…³é”®å­ä¸»é¢˜å’Œç ”ç©¶é—®é¢˜:\n"
        
        # æŒ‰å­ä¸»é¢˜åˆ†ç»„ç ”ç©¶é—®é¢˜
        for i, sub_topic in enumerate(research_plan.sub_topics, 1):
            plan_str += f"\n{i}. {sub_topic}\n"
            plan_str += "   ç ”ç©¶é—®é¢˜:\n"
            # æ‰¾å‡ºå±äºå½“å‰å­ä¸»é¢˜çš„ç ”ç©¶é—®é¢˜
            topic_questions = [q for q in research_plan.research_questions if q.startswith(f"{sub_topic}ï¼š")]
            if not topic_questions:
                # å¦‚æœæ²¡æœ‰ä¸¥æ ¼åŒ¹é…çš„ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                topic_questions = [q for q in research_plan.research_questions if sub_topic in q]
            for j, question in enumerate(topic_questions, 1):
                # ç§»é™¤ã€Œå­ä¸»é¢˜ï¼šã€å‰ç¼€ï¼Œåªæ˜¾ç¤ºé—®é¢˜æœ¬èº«
                question_text = question.split("ï¼š", 1)[-1] if "ï¼š" in question else question
                plan_str += f"   {i}.{j}. {question_text}\n"
        
        plan_str += f"\nç†ç”±: {research_plan.rationale}"
    
    # æ ¹æ®æ¨¡å¼è®¾ç½®ä¸åŒçš„æç¤ºè¯
    if is_targeted_mode:
        # é’ˆå¯¹æ€§æ¨¡å¼ï¼šä»…é’ˆå¯¹æœªå›ç­”çš„é—®é¢˜ç”ŸæˆæŸ¥è¯¢
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘è¿è¡Œæ¨¡å¼: é’ˆå¯¹æ€§ï¼ˆTargetedï¼‰")
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘æœªå›ç­”é—®é¢˜æ•°é‡: {len(unanswered_questions)}")
        for idx, question in enumerate(unanswered_questions[:3], 1):
            logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘  æœªå›ç­”é—®é¢˜ {idx}: {question[:100]}...")
        
        # é’ˆå¯¹æ€§æ¨¡å¼ï¼šæ¯ä¸ªé—®é¢˜ç”Ÿæˆ1-2ä¸ªæŸ¥è¯¢ï¼Œæ€»æ•°ä¸è¶…è¿‡é…ç½®çš„ä¸Šé™
        max_queries = min(len(unanswered_questions) * 2, state.get("initial_search_query_count", 3))
        
        unanswered_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(unanswered_questions)])
        mode_instruction = f"""**é’ˆå¯¹æ€§æ¨¡å¼ (Targeted Mode):**
- å½“å‰å­˜åœ¨ {len(unanswered_questions)} ä¸ªæœªå……åˆ†å›ç­”çš„ç ”ç©¶é—®é¢˜
- ä½ çš„ä»»åŠ¡æ˜¯**ä»…é’ˆå¯¹ä»¥ä¸‹æœªå›ç­”é—®é¢˜**ç”Ÿæˆç²¾å‡†çš„æœç´¢æŸ¥è¯¢
- æ¯ä¸ªé—®é¢˜ç”Ÿæˆ 1-2 ä¸ªæŸ¥è¯¢ï¼Œé¿å…é‡å¤
- æŸ¥è¯¢åº”ç›´æ¥æœåŠ¡äºå›ç­”è¿™äº›å…·ä½“é—®é¢˜
- ä¸è¦ç”Ÿæˆè¶…å‡ºæ­¤æ¸…å•èŒƒå›´çš„æŸ¥è¯¢

æœªå›ç­”çš„é—®é¢˜æ¸…å•ï¼š
{unanswered_text}
"""
    else:
        # é¦–æ¬¡è¿è¡Œæ¨¡å¼ï¼šåŸºäºå®Œæ•´ç ”ç©¶è®¡åˆ’ç”Ÿæˆåˆå§‹æŸ¥è¯¢
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘è¿è¡Œæ¨¡å¼: é¦–æ¬¡ï¼ˆInitialï¼‰")
        initial_count = state.get("initial_search_query_count")
        if initial_count is None:
            initial_count = 3
            state["initial_search_query_count"] = initial_count
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡: {initial_count}")
        if research_plan:
            logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘åŸºäºæ–¹æ¡ˆ '{research_plan.research_topic}' ç”ŸæˆæŸ¥è¯¢")
        
        max_queries = initial_count
        mode_instruction = """**é¦–æ¬¡è¿è¡Œæ¨¡å¼ (Initial Mode):**
- è¿™æ˜¯ç¬¬ä¸€æ¬¡ç”Ÿæˆæœç´¢æŸ¥è¯¢
- è¯·åŸºäºå®Œæ•´çš„ç ”ç©¶è®¡åˆ’ (Research Plan) ç”Ÿæˆå¤šæ ·åŒ–çš„åˆå§‹æŸ¥è¯¢
- æŸ¥è¯¢åº”è¦†ç›–ç ”ç©¶è®¡åˆ’ä¸­çš„å„ä¸ªå­ä¸»é¢˜å’Œå…³é”®é—®é¢˜
- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.
- Each query should focus on one specific aspect of the original question.
- Queries should be diverse, if the topic is broad, generate more than 1 query.
- Don't generate multiple similar queries, 1 is enough.
"""
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ç›®æ ‡æŸ¥è¯¢æ•°é‡: {max_queries}")
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ç ”ç©¶ä¸»é¢˜: {research_topic[:200]}...")
    
    formatted_prompt = query_writer_instructions.format(
        current_date=get_current_date(),
        research_topic=research_topic,
        research_plan=plan_str,
        mode_instruction=mode_instruction,
        number_queries=max_queries,
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘è°ƒç”¨ LLM ç”ŸæˆæŸ¥è¯¢...")
    result = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
        node_name="generate_query",
        gemini_model=reasoning_model,
        temperature=1.0,
        structured_output_type=SearchQueryList,
        connection_id=connection_id
    )
    # è¿”å›åå†æ¬¡æ£€æŸ¥å–æ¶ˆï¼Œé¿å…åç»­æ—¥å¿—ç»§ç»­è¾“å‡º
    check_cancellation_and_raise(connection_id)
    
    query_count = len(result.query) if result.query else 0
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘æˆåŠŸç”Ÿæˆ {query_count} ä¸ªæœç´¢æŸ¥è¯¢")
    for idx, query_item in enumerate(result.query[:5], 1):  # åªè®°å½•å‰5ä¸ª
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘  æŸ¥è¯¢ {idx}: {query_item[:100]}...")
    
    # è¿”å›ä¸¤ä¸ªå­—æ®µï¼šsearch_query ç”¨äºç´¯ç§¯ï¼ˆå†å²è®°å½•ï¼‰ï¼Œnew_search_query ç”¨äºæœ¬è½®æ‰§è¡Œ
    return {"search_query": result.query, "new_search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    # åªå¤„ç†æœ¬è½®æ–°ç”Ÿæˆçš„æŸ¥è¯¢ï¼Œé¿å…é‡å¤æ‰§è¡Œå†å²æŸ¥è¯¢
    new_queries = state.get("new_search_query", [])
    query_count = len(new_queries)
    logger.info(f"ã€èŠ‚ç‚¹: continue_to_web_researchã€‘å‡†å¤‡åˆ†å‘ {query_count} ä¸ªæœç´¢ä»»åŠ¡åˆ° web_research èŠ‚ç‚¹")
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(new_queries)
    ]


async def bocha_web_search(query: str, count: int = 10) -> Dict[str, Any]:
    """
    ä½¿ç”¨åšæŸ¥æœç´¢ API è¿›è¡Œç½‘é¡µæœç´¢ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰ã€‚

    å‚æ•°:
    - query: æœç´¢å…³é”®è¯
    - count: è¿”å›çš„æœç´¢ç»“æœæ•°é‡

    è¿”å›:
    - åŒ…å«æœç´¢ç»“æœå’Œæ ¼å¼åŒ–æ–‡æœ¬çš„å­—å…¸
    """
    if not settings.BOCHA_API_KEY:
        raise ValueError("BOCHA_API_KEY is not set in environment variables")

    url = 'https://api.bochaai.com/v1/web-search'
    headers = {
        'Authorization': f'Bearer {settings.BOCHA_API_KEY}',
        'Content-Type': 'application/json'
    }
    data = {
        "query": query,
        "freshness": "noLimit",  # æœç´¢çš„æ—¶é—´èŒƒå›´
        "summary": True,  # æ˜¯å¦è¿”å›é•¿æ–‡æœ¬æ‘˜è¦
        "count": count
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=data)
        
            if response.status_code == 200:
                json_response = response.json()
                if json_response.get("code") != 200 or not json_response.get("data"):
                    error_msg = json_response.get("msg", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"åšæŸ¥æœç´¢APIè¯·æ±‚å¤±è´¥: {error_msg}")
                    return {
                        "webpages": [],
                        "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒåŸå› æ˜¯: {error_msg}"
                    }
                
                webpages = json_response.get("data", {}).get("webPages", {}).get("value", [])
                if not webpages:
                    logger.warning(f"åšæŸ¥æœç´¢APIè¿”å›ç©ºç»“æœï¼ŒæŸ¥è¯¢: {query[:100]}...")
                    return {
                        "webpages": [],
                        "formatted_text": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
                    }
                
                logger.info(f"åšæŸ¥æœç´¢APIæˆåŠŸè¿”å› {len(webpages)} ä¸ªç»“æœï¼ŒæŸ¥è¯¢: {query[:100]}...")
                formatted_text = format_bocha_search_results(webpages)
                return {
                    "webpages": webpages,
                    "formatted_text": formatted_text
                }
            else:
                error_msg = f"çŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {response.text}"
                logger.error(f"åšæŸ¥æœç´¢APIè¯·æ±‚å¤±è´¥: {error_msg}")
                return {
                    "webpages": [],
                    "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼Œ{error_msg}"
                }
    except Exception as e:
        logger.error(f"åšæŸ¥æœç´¢APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return {
            "webpages": [],
            "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒåŸå› æ˜¯ï¼š{str(e)}"
        }


async def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """
    ä½¿ç”¨åšæŸ¥æœç´¢ API è¿›è¡Œç½‘é¡µç ”ç©¶ï¼Œå¹¶è¿›è¡Œæ·±åº¦æŠ“å–å’Œæ­£æ–‡æå–ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰ã€‚
    """
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    search_query = state["search_query"]
    search_id = state["id"]
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡ ID={search_id}")
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢æŸ¥è¯¢: {search_query[:200]}...")
    
    # è°ƒç”¨åšæŸ¥æœç´¢ APIï¼ˆå¼‚æ­¥ï¼‰
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘è°ƒç”¨åšæŸ¥æœç´¢ API...")
    search_result = await bocha_web_search(query=search_query, count=10)
    
    webpages = search_result.get("webpages", [])
    formatted_text = search_result.get("formatted_text", "")
    
    webpage_count = len(webpages) if webpages else 0
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {webpage_count} ä¸ªç½‘é¡µç»“æœ")
    
    if webpage_count > 0:
        logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å‰3ä¸ªç»“æœæ ‡é¢˜:")
        for idx, page in enumerate(webpages[:3], 1):
            logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘  {idx}. {page.get('name', 'N/A')[:100]}")
    
    # ========== æ·±åº¦æŠ“å–é€»è¾‘å¼€å§‹ ==========
    
    # é€‰å– Top-K ç½‘é¡µè¿›è¡Œæ·±åº¦æŠ“å–
    top_k = min(settings.WEB_SCRAPE_TOP_K, len(webpages))
    top_pages = webpages[:top_k]
    top_urls = [p.get("url") for p in top_pages if p.get("url")]
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å‡†å¤‡æ·±åº¦æŠ“å– Top-{top_k} ç½‘é¡µ...")
    
    # å¹¶å‘æŠ“å–ç½‘é¡µå¹¶æå–æ­£æ–‡
    deep_docs = []
    if top_urls:
        try:
            scraped_results = await scrape_webpages(
                urls=top_urls,
                timeout=settings.WEB_SCRAPE_TIMEOUT,
                concurrency=settings.WEB_SCRAPE_CONCURRENCY,
                max_per_doc_chars=settings.WEB_SCRAPE_MAX_PER_DOC_CHARS,
                user_agent=settings.WEB_SCRAPE_USER_AGENT,
            )
            
            # ç»„è£…æ·±åº¦æ–‡æ¡£åˆ—è¡¨ï¼ˆä¿æŒç¼–å·ä¸ top_pages ä¸€è‡´ï¼‰
            url_to_text = {url: text for url, text in scraped_results}
            
            for i, page in enumerate(top_pages, start=1):
                url = page.get("url", "")
                if url in url_to_text:
                    title = page.get("name", f"æ¥æº{i}")
                    text = url_to_text[url]
                    deep_docs.append((i, title, url, text))
            
            logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æˆåŠŸæ·±åº¦æŠ“å– {len(deep_docs)}/{top_k} ä¸ªç½‘é¡µ")
            
        except Exception as e:
            logger.error(f"ã€èŠ‚ç‚¹: web_researchã€‘æ·±åº¦æŠ“å–å¤±è´¥: {e}", exc_info=True)
    
    # æ„å»º LLM ä¸Šä¸‹æ–‡
    context_for_llm = ""
    
    if deep_docs:
        # ä½¿ç”¨æ·±åº¦æŠ“å–çš„æ­£æ–‡ä½œä¸ºä¸Šä¸‹æ–‡
        deep_context_parts = []
        for idx, title, url, text in deep_docs:
            deep_context_parts.append(
                f"[{idx}] æ ‡é¢˜: {title}\n"
                f"URL: {url}\n"
                f"æ­£æ–‡:\n{text}\n"
                f"---"
            )
        deep_context = "\n".join(deep_context_parts)
        
        # æ§åˆ¶æ€»é•¿åº¦
        deep_context = clean_and_truncate(
            deep_context, 
            settings.WEB_SCRAPE_MAX_TOTAL_CHARS
        )
        context_for_llm = deep_context
        
        logger.info(
            f"ã€èŠ‚ç‚¹: web_researchã€‘ä½¿ç”¨æ·±åº¦æ­£æ–‡ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œ"
            f"æ€»é•¿åº¦: {len(context_for_llm)} å­—ç¬¦"
        )
    else:
        # å›é€€åˆ°åšæŸ¥æœç´¢çš„æ‘˜è¦
        context_for_llm = formatted_text
        logger.warning(
            f"ã€èŠ‚ç‚¹: web_researchã€‘æ·±åº¦æŠ“å–å¤±è´¥æˆ–æ— ç»“æœï¼Œ"
            f"å›é€€ä½¿ç”¨åšæŸ¥æœç´¢æ‘˜è¦"
        )
    
    # ========== æ·±åº¦æŠ“å–é€»è¾‘ç»“æŸ ==========
    
    # ä½¿ç”¨ Gemini å¯¹æœç´¢ç»“æœè¿›è¡Œæ€»ç»“å’Œæ•´ç†
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹ä½¿ç”¨ LLM æ€»ç»“æœç´¢ç»“æœ...")
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=search_query,
    )

    # å°†æœç´¢ç»“æœæ·»åŠ åˆ°æç¤ºè¯ä¸­ï¼Œå¹¶æç¤º LLM ä½¿ç”¨å¼•ç”¨ç¼–å·
    search_context = (
        f"\n\næœç´¢æŸ¥è¯¢: {search_query}\n"
        f"ä»…åŸºäºä»¥ä¸‹ç½‘é¡µæ­£æ–‡å†…å®¹è¿›è¡Œä¸¥è°¨æ€»ç»“ï¼Œå¹¶åœ¨æ¯æ¡äº‹å®åä½¿ç”¨ [ç¼–å·] æ ‡æ³¨æ¥æºï¼š\n"
        f"{context_for_llm}"
    )
    full_prompt = formatted_prompt + search_context

    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘è°ƒç”¨ LLM APIï¼Œæç¤ºè¯é•¿åº¦: {len(full_prompt)} å­—ç¬¦")
    llm_response = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(full_prompt),
        node_name="web_research",
        gemini_model=settings.GEMINI_MODEL,
        temperature=0,
        connection_id=connection_id
    )
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘LLM æ€»ç»“å®Œæˆï¼Œå“åº”é•¿åº¦: {len(llm_response.content)} å­—ç¬¦")
    
    # å¤„ç†å¼•ç”¨å’Œæ¥æº
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹å¤„ç†å¼•ç”¨å’Œæ¥æº...")
    
    # å¦‚æœæœ‰æ·±åº¦æŠ“å–çš„ç»“æœï¼Œä½¿ç”¨ top_pagesï¼›å¦åˆ™ä½¿ç”¨å…¨éƒ¨ webpages
    pages_for_citation = top_pages if deep_docs else webpages
    
    resolved_urls = resolve_urls(pages_for_citation, search_id)
    citations = get_citations_from_bocha(
        pages_for_citation, 
        resolved_urls, 
        llm_response.content
    )
    modified_text = insert_citation_markers(llm_response.content, citations)
    
    # sources_gathered: ä»…åŒ…å«æ·±åº¦æŠ“å–çš„æ¥æºï¼ˆç”¨äºå¼•ç”¨ï¼‰
    sources_gathered = [
        item for citation in citations 
        for item in citation["segments"]
    ]
    
    # all_sources_gathered: åŒ…å«æ‰€æœ‰æœç´¢åˆ°çš„æ¥æºï¼ˆåŒ…æ‹¬é top-kï¼‰
    all_resolved_urls = resolve_urls(webpages, search_id)
    all_sources = []
    for page in webpages:
        url = page.get("url", "")
        if url:
            title = page.get("name", "")
            site_name = page.get("siteName", "")
            all_sources.append({
                "label": title[:50] if title else site_name[:50] if site_name else "æ¥æº",
                "shortUrl": all_resolved_urls.get(url, url),
                "value": url,
            })
    
    logger.info(
        f"ã€èŠ‚ç‚¹: web_researchã€‘å¤„ç†å®Œæˆï¼Œ"
        f"ç”Ÿæˆ {len(citations)} ä¸ªå¼•ç”¨ï¼Œ"
        f"{len(sources_gathered)} ä¸ªæ·±åº¦æ¥æºï¼Œ"
        f"{len(all_sources)} ä¸ªå€™é€‰æ¥æº"
    )
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢ä»»åŠ¡ ID={search_id} å®Œæˆ")

    return {
        "sources_gathered": sources_gathered,
        "all_sources_gathered": all_sources,  # ä¿å­˜æ‰€æœ‰æœç´¢åˆ°çš„èµ„æº
        "search_query": [search_query],
        "web_research_result": [modified_text],
    }


async def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    loop_count = state["research_loop_count"]
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘å¼€å§‹åæ€ï¼Œç ”ç©¶å¾ªç¯æ¬¡æ•°: {loop_count}")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    web_research_results = state.get("web_research_result", [])
    search_queries = state.get("search_query", [])
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘å½“å‰å·²æœ‰ {len(web_research_results)} ä¸ªæœç´¢ç»“æœï¼Œ{len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")

    # è·å–ç ”ç©¶è®¡åˆ’å¹¶æ ¼å¼åŒ–
    research_plan = state.get("research_plan")
    plan_str = "æ— ç‰¹å®šç ”ç©¶è®¡åˆ’"
    if research_plan:
        plan_str = f"ç ”ç©¶ä¸»é¢˜: {research_plan.research_topic}\n\n"
        plan_str += "å­ä¸»é¢˜:\n"
        for i, sub_topic in enumerate(research_plan.sub_topics, 1):
            plan_str += f"{i}. {sub_topic}\n"
        plan_str += "\nç ”ç©¶é—®é¢˜ (Research Questions):\n"
        for i, question in enumerate(research_plan.research_questions, 1):
            plan_str += f"{i}. {question}\n"
        plan_str += f"\nç†ç”±: {research_plan.rationale}"
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘ä½¿ç”¨ç ”ç©¶è®¡åˆ’è¿›è¡Œå¯¹ç…§è¯„ä¼°ï¼ŒåŒ…å« {len(research_plan.research_questions)} ä¸ªé—®é¢˜")

    formatted_prompt = reflection_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        research_plan=plan_str,
        loop_count=loop_count,
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    
    logger.info("ã€èŠ‚ç‚¹: reflectionã€‘è°ƒç”¨ LLM è¿›è¡Œåæ€è¯„ä¼°ï¼ˆå¯¹ç…§ç ”ç©¶è®¡åˆ’ï¼‰...")
    result = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
        node_name="reflection",
        gemini_model=reasoning_model,
        temperature=1.0,
        structured_output_type=Reflection,
        connection_id=connection_id
    )

    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘=== ä¿¡æ¯å……è¶³æ€§è¯„ä¼°ç»“æœ ===")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  å½“å‰å¾ªç¯: ç¬¬ {loop_count} è½®")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  ä¿¡æ¯æ˜¯å¦å……è¶³: {result.is_sufficient}")
    
    if result.is_sufficient:
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  âœ… è¯„ä¼°ç»“æœ: ä¿¡æ¯å·²å……è¶³ï¼Œå¯ä»¥å¼€å§‹ç”ŸæˆæŠ¥å‘Š")
    else:
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  âš ï¸  è¯„ä¼°ç»“æœ: ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç»§ç»­ç ”ç©¶")
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  çŸ¥è¯†ç¼ºå£: {result.knowledge_gap[:200] if result.knowledge_gap else 'N/A'}...")
        unanswered_count = len(result.unanswered_questions) if result.unanswered_questions else 0
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  æœªå›ç­”é—®é¢˜æ•°é‡: {unanswered_count}")
        if unanswered_count > 0:
            for idx, question in enumerate(result.unanswered_questions[:3], 1):  # åªè®°å½•å‰3ä¸ª
                logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘    æœªå›ç­”é—®é¢˜ {idx}: {question[:100]}...")

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "unanswered_questions": result.unanswered_questions,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
        "max_research_loops": state.get("max_research_loops", 5),  # ä¼ é€’æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œé»˜è®¤5
    }


def evaluate_research(state: ReflectionState, config: RunnableConfig) -> str:
    max_research_loops = state.get("max_research_loops", 5)  # é»˜è®¤5æ¬¡å¾ªç¯
    loop_count = state["research_loop_count"]
    is_sufficient = state["is_sufficient"]
    
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘=== ç ”ç©¶çŠ¶æ€å†³ç­– ===")
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘  å½“å‰å¾ªç¯æ¬¡æ•°: {loop_count}/{max_research_loops}")
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘  ä¿¡æ¯æ˜¯å¦å……è¶³: {is_sufficient}")
    
    if state["is_sufficient"]:
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘âœ… å†³ç­–: ä¿¡æ¯å·²å……è¶³ï¼Œç»“æŸè°ƒæŸ¥å¾ªç¯")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: å¼€å§‹è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹")
        return "assess_content_quality"
    elif state["research_loop_count"] >= max_research_loops:
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘âš ï¸  å†³ç­–: å·²è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° ({max_research_loops})ï¼Œå¼ºåˆ¶ç»“æŸ")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: åŸºäºç°æœ‰ä¿¡æ¯ç”ŸæˆæŠ¥å‘Š")
        return "assess_content_quality"
    else:
        unanswered_questions = state.get("unanswered_questions", [])
        unanswered_count = len(unanswered_questions)
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘ğŸ”„ å†³ç­–: ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­ç¬¬ {loop_count + 1} è½®è°ƒæŸ¥")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: ç”Ÿæˆé’ˆå¯¹ {unanswered_count} ä¸ªæœªå›ç­”é—®é¢˜çš„æ–°æŸ¥è¯¢")
        return "generate_query"


async def assess_content_quality(state: OverallState, config: RunnableConfig):
    """å†…å®¹è´¨é‡è¯„ä¼°èŠ‚ç‚¹ã€‚"""
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘å¼€å§‹å†…å®¹è´¨é‡è¯„ä¼°")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    formatted_prompt = content_quality_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    result = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
        node_name="assess_content_quality",
        gemini_model=reasoning_model,
        temperature=0.3,
        structured_output_type=ContentQualityAssessment,
        connection_id=connection_id
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘è´¨é‡è¯„åˆ†: {result.quality_score}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘å†…å®¹ç©ºç™½æ•°é‡: {len(result.content_gaps)}")
    
    return {
        "content_quality": {
            "quality_score": result.quality_score,
            "reliability_assessment": result.reliability_assessment,
            "content_gaps": result.content_gaps,
            "improvement_suggestions": result.improvement_suggestions
        }
    }


async def verify_facts(state: OverallState, config: RunnableConfig):
    """äº‹å®éªŒè¯èŠ‚ç‚¹ã€‚"""
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘å¼€å§‹äº‹å®éªŒè¯")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = fact_verification_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    async def ainvoke_with_method(llm: ChatOpenAI):
        """å¼‚æ­¥è°ƒç”¨å¸¦ method å‚æ•°çš„ structured_output."""
        structured_llm = llm.with_structured_output(
            FactVerification,
            method="json_schema",
            include_raw=False
        )
        return await structured_llm.ainvoke(formatted_prompt)
    
    result = await invoke_llm_with_fallback(
        invoke_func=ainvoke_with_method,
        node_name="verify_facts",
        gemini_model=reasoning_model,
        temperature=0.1,
        connection_id=connection_id
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘éªŒè¯ç½®ä¿¡åº¦: {result.confidence_score}")
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘å·²éªŒè¯äº‹å®æ•°é‡: {len(result.verified_facts_text)}")
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘äº‰è®®å£°æ˜æ•°é‡: {len(result.disputed_claims_text)}")
    
    # å°†æ‰å¹³åŒ–çš„åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    verified_facts_dicts = [
        {"fact": fact, "source": source} 
        for fact, source in zip(result.verified_facts_text, result.verified_facts_sources)
    ]
    disputed_claims_dicts = [
        {"claim": claim, "reason": reason} 
        for claim, reason in zip(result.disputed_claims_text, result.disputed_claims_reasons)
    ]
    
    return {
        "fact_verification": {
            "verified_facts": verified_facts_dicts,
            "disputed_claims": disputed_claims_dicts,
            "verification_sources": result.verification_sources,
            "confidence_score": result.confidence_score
        }
    }


async def assess_relevance(state: OverallState, config: RunnableConfig):
    """ç›¸å…³æ€§è¯„ä¼°èŠ‚ç‚¹ã€‚"""
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘å¼€å§‹ç›¸å…³æ€§è¯„ä¼°")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    formatted_prompt = relevance_assessment_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    result = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
        node_name="assess_relevance",
        gemini_model=reasoning_model,
        temperature=0.2,
        structured_output_type=RelevanceAssessment,
        connection_id=connection_id
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ç›¸å…³æ€§è¯„åˆ†: {result.relevance_score}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘è¦†ç›–å…³é”®ä¸»é¢˜æ•°é‡: {len(result.key_topics_covered)}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ç¼ºå¤±ä¸»é¢˜æ•°é‡: {len(result.missing_topics)}")
    
    return {
        "relevance_assessment": {
            "relevance_score": result.relevance_score,
            "key_topics_covered": result.key_topics_covered,
            "missing_topics": result.missing_topics,
            "content_alignment": result.content_alignment
        }
    }


async def optimize_summary(state: OverallState, config: RunnableConfig):
    """æ‘˜è¦ä¼˜åŒ–èŠ‚ç‚¹ã€‚"""
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å¼€å§‹æ‘˜è¦ä¼˜åŒ–")
    
    # è·å–åŸå§‹æ‘˜è¦
    original_summary = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = summary_optimization_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        original_summary=original_summary,
        quality_assessment=str(state.get("content_quality", {})),
        fact_verification=str(state.get("fact_verification", {})),
        relevance_assessment=str(state.get("relevance_assessment", {}))
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    result = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
        node_name="optimize_summary",
        gemini_model=reasoning_model,
        temperature=0.3,
        structured_output_type=SummaryOptimization,
        connection_id=connection_id
    )
    
    # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†
    quality_score = state.get("content_quality", {}).get("quality_score", 0.5)
    fact_confidence = state.get("fact_verification", {}).get("confidence_score", 0.5)
    relevance_score = state.get("relevance_assessment", {}).get("relevance_score", 0.5)
    final_confidence = (quality_score + fact_confidence + relevance_score) / 3
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å…³é”®æ´å¯Ÿæ•°é‡: {len(result.key_insights)}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å¯è¡Œå»ºè®®æ•°é‡: {len(result.actionable_items)}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘ç½®ä¿¡åº¦ç­‰çº§: {result.confidence_level}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†: {final_confidence:.3f}")
    
    return {
        "summary_optimization": {
            "key_insights": result.key_insights,
            "actionable_items": result.actionable_items,
            "confidence_level": result.confidence_level
        },
        "final_confidence_score": final_confidence
    }


async def generate_verification_report(state: OverallState, config: RunnableConfig):
    """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘ŠèŠ‚ç‚¹ã€‚"""
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_verification_reportã€‘å¼€å§‹ç”ŸæˆéªŒè¯æŠ¥å‘Š")
    
    # ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š
    quality_data = state.get("content_quality", {})
    fact_data = state.get("fact_verification", {})
    relevance_data = state.get("relevance_assessment", {})
    optimization_data = state.get("summary_optimization", {})
    
    report = f"""
# ç ”ç©¶è´¨é‡éªŒè¯æŠ¥å‘Š

## å†…å®¹è´¨é‡è¯„ä¼°
- **è´¨é‡è¯„åˆ†**: {quality_data.get('quality_score', 'N/A'):.2f}/1.0
- **å¯é æ€§è¯„ä¼°**: {quality_data.get('reliability_assessment', 'N/A')}
- **å†…å®¹ç©ºç™½**: {', '.join(quality_data.get('content_gaps', [])) if quality_data.get('content_gaps') else 'æ— æ˜æ˜¾ç©ºç™½'}
- **æ”¹è¿›å»ºè®®**: {', '.join(quality_data.get('improvement_suggestions', [])) if quality_data.get('improvement_suggestions') else 'æ— ç‰¹åˆ«å»ºè®®'}

## äº‹å®éªŒè¯ç»“æœ
- **éªŒè¯ç½®ä¿¡åº¦**: {fact_data.get('confidence_score', 'N/A'):.2f}/1.0
- **å·²éªŒè¯äº‹å®æ•°é‡**: {len(fact_data.get('verified_facts', []))}
- **äº‰è®®å£°æ˜æ•°é‡**: {len(fact_data.get('disputed_claims', []))}
- **éªŒè¯æ¥æº**: {', '.join(fact_data.get('verification_sources', [])) if fact_data.get('verification_sources') else 'å¤šä¸ªæ¥æº'}

## ç›¸å…³æ€§è¯„ä¼°
- **ç›¸å…³æ€§è¯„åˆ†**: {relevance_data.get('relevance_score', 'N/A'):.2f}/1.0
- **å·²è¦†ç›–å…³é”®ä¸»é¢˜**: {', '.join(relevance_data.get('key_topics_covered', [])) if relevance_data.get('key_topics_covered') else 'N/A'}
- **ç¼ºå¤±ä¸»é¢˜**: {', '.join(relevance_data.get('missing_topics', [])) if relevance_data.get('missing_topics') else 'æ— æ˜æ˜¾ç¼ºå¤±'}
- **å†…å®¹ä¸€è‡´æ€§**: {relevance_data.get('content_alignment', 'N/A')}

## æ‘˜è¦ä¼˜åŒ–ç»“æœ
- **ç½®ä¿¡åº¦ç­‰çº§**: {optimization_data.get('confidence_level', 'N/A')}
- **å…³é”®æ´å¯Ÿæ•°é‡**: {len(optimization_data.get('key_insights', []))}
- **å¯è¡Œå»ºè®®æ•°é‡**: {len(optimization_data.get('actionable_items', []))}

## ç»¼åˆè¯„ä¼°
- **æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†**: {state.get('final_confidence_score', 0):.3f}/1.0
"""
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_verification_reportã€‘éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    return {
        "verification_report": report
    }


async def finalize_answer(state: OverallState, config: RunnableConfig):
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œè¿”å›é«˜åº¦å›´ç»•ç”¨æˆ·æé—®çš„è°ƒæŸ¥ç ”ç©¶æŠ¥å‘Šã€‚"""
    # è·å–connection_idç”¨äºå–æ¶ˆæ£€æŸ¥
    connection_id = None
    if config:
        if hasattr(config, 'configurable') and config.configurable:
            connection_id = config.configurable.get("connection_id")
        elif isinstance(config, dict):
            connection_id = config.get("configurable", {}).get("connection_id")
    
    # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation_and_raise(connection_id)
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å¼€å§‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    web_research_results = state.get("web_research_result", [])
    sources_gathered = state.get("sources_gathered", [])
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æ±‡æ€» {len(web_research_results)} ä¸ªæœç´¢ç»“æœï¼Œ{len(sources_gathered)} ä¸ªæ•°æ®æº")

    # è·å–æ‰€æœ‰åŸå§‹ææ–™
    summaries = "\n---\n\n".join(web_research_results)
    
    # è·å–ä¸Šä¸€æ­¥çš„ç»“æ„åŒ–æ´å¯Ÿ
    optimization_data = state.get("summary_optimization", {})
    key_insights = optimization_data.get("key_insights", [])
    actionable_items = optimization_data.get("actionable_items", [])
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æ ¸å¿ƒæ´å¯Ÿæ•°é‡: {len(key_insights)}")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å¯è¡Œå»ºè®®æ•°é‡: {len(actionable_items)}")
    
    # æ„å»ºå¢å¼ºçš„æç¤ºè¯ï¼Œå°†ç»“æ„åŒ–æ´å¯Ÿæ³¨å…¥
    prompt_enhancement = ""
    if key_insights or actionable_items:
        prompt_enhancement = "\n\n---\n\n**ä»¥ä¸‹æ˜¯åŸºäºç ”ç©¶ææ–™æç‚¼å‡ºçš„æ ¸å¿ƒæ´å¯Ÿå’Œå»ºè®®ï¼Œè¯·å°†å®ƒä»¬ä½œä¸ºæŠ¥å‘Šçš„é‡ç‚¹ï¼Œåœ¨æŠ¥å‘Šä¸­è¯¦ç»†å±•å¼€è®ºè¿°ï¼š**\n\n"
        
        if key_insights:
            prompt_enhancement += "**æ ¸å¿ƒæ´å¯Ÿ (Key Insights):**\n"
            for i, insight in enumerate(key_insights, 1):
                prompt_enhancement += f"{i}. {insight}\n"
            prompt_enhancement += "\n"
        
        if actionable_items:
            prompt_enhancement += "**å¯è¡Œå»ºè®® (Actionable Items):**\n"
            for i, item in enumerate(actionable_items, 1):
                prompt_enhancement += f"{i}. {item}\n"
    
    # ä½¿ç”¨ answer_instructions æ¥æ’°å†™æŠ¥å‘Š
    formatted_prompt = answer_instructions.format(
        current_date=get_current_date(),
        research_topic=get_research_topic(state["messages"]),
        summaries=summaries + prompt_enhancement  # å°†æ´å¯Ÿæ³¨å…¥æç¤ºè¯
    )
    
    logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘è°ƒç”¨ LLM ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š...")
    
    # *** å…³é”®ï¼šè¿™é‡Œä¸ä½¿ç”¨ .with_structured_output()ï¼Œç›´æ¥ç”Ÿæˆçº¯æ–‡æœ¬æŠ¥å‘Š ***
    result = await invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.ainvoke(formatted_prompt),
        node_name="finalize_answer",
        gemini_model=reasoning_model,
        temperature=0.2,
        connection_id=connection_id
    )
    final_report = result.content  # è¿™å°±æ˜¯çº¯ Markdown æŠ¥å‘Š
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘LLM ç”Ÿæˆå®Œæˆï¼ŒæŠ¥å‘Šé•¿åº¦: {len(final_report)} å­—ç¬¦")
    
    # å¤„ç†æ•°æ®æºå¼•ç”¨ï¼ˆæ”¹è¿›ç‰ˆï¼šæ‰«æå¼•ç”¨ç¼–å·å¹¶ç”Ÿæˆå‚è€ƒæ¥æºåˆ—è¡¨ï¼‰
    logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘å¤„ç†æ•°æ®æºå¼•ç”¨...")
    import re
    
    # 1. æ‰«ææŠ¥å‘Šä¸­çš„æ‰€æœ‰å¼•ç”¨ç¼–å· [1], [2], [3] ç­‰
    citation_pattern = re.compile(r'\[(\d+)\]')
    found_citations = set(citation_pattern.findall(final_report))
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æ‰«æåˆ° {len(found_citations)} ä¸ªå¼•ç”¨ç¼–å·: {sorted(found_citations, key=int)}")
    
    # 2. å°† shortUrl æ›¿æ¢ä¸ºå®é™… URL
    enhanced_content = final_report
    for source in sources_gathered:
        if source["shortUrl"] in enhanced_content:
            enhanced_content = enhanced_content.replace(source["shortUrl"], source["value"])
    
    # 3. æ„å»ºå¼•ç”¨ç¼–å·åˆ°æ¥æºçš„æ˜ å°„ï¼ˆåŸºäºé¡ºåºï¼‰
    citation_to_source = {}
    unique_sources: List[Dict[str, Any]] = []
    
    # æŒ‰å¼•ç”¨ç¼–å·æ’åºæ¥æºï¼ˆåŸºäº shortUrl ä¸­çš„ç¼–å·ï¼‰
    def extract_citation_num(source: Dict[str, Any]) -> int:
        """ä» shortUrl ä¸­æå–å¼•ç”¨ç¼–å·"""
        short_url = source.get("shortUrl", "")
        # shortUrl æ ¼å¼: https://vertexaisearch.cloud.google.com/id/{search_id}-{idx}
        match = re.search(r'/id/\d+-(\d+)$', short_url)
        if match:
            return int(match.group(1))
        return 999999  # å¦‚æœæ— æ³•æå–ï¼Œæ”¾åˆ°æœ€å
    
    # æŒ‰ç…§å¼•ç”¨ç¼–å·æ’åºæ¥æº
    sorted_sources = sorted(sources_gathered, key=extract_citation_num)
    
    # ä¸ºæ¯ä¸ªæ¥æºåˆ†é…å¼•ç”¨ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
    for idx, source in enumerate(sorted_sources, start=1):
        citation_num = str(idx)
        citation_to_source[citation_num] = source
        unique_sources.append(source)
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å…±æœ‰ {len(unique_sources)} ä¸ªæ•°æ®æº")
    
    # 4. åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ "å‚è€ƒæ¥æº"åˆ—è¡¨ï¼ˆå¦‚æœæŠ¥å‘Šä¸­æœ‰å¼•ç”¨ç¼–å·ï¼‰
    if found_citations:
        logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ å‚è€ƒæ¥æºåˆ—è¡¨...")
        
        # æ£€æŸ¥æŠ¥å‘Šæ˜¯å¦å·²æœ‰"å‚è€ƒæ¥æº"ã€"å¼•ç”¨"ã€"æ¥æº" ç­‰æ ‡é¢˜
        has_references = bool(re.search(r'#+\s*(å‚è€ƒæ¥æº|å¼•ç”¨|æ¥æº|å‚è€ƒèµ„æ–™|References)', enhanced_content, re.IGNORECASE))
        
        if not has_references:
            # å¦‚æœæ²¡æœ‰ï¼Œæ·»åŠ å‚è€ƒæ¥æºåˆ—è¡¨
            enhanced_content += "\n\n---\n\n## å‚è€ƒæ¥æº\n\n"
            
            # æŒ‰å¼•ç”¨ç¼–å·æ’åº
            sorted_citations = sorted([int(c) for c in found_citations])
            
            for citation_num in sorted_citations:
                citation_str = str(citation_num)
                if citation_str in citation_to_source:
                    source = citation_to_source[citation_str]
                    label = source.get("label", f"æ¥æº {citation_num}")
                    url = source.get("value", "")
                    enhanced_content += f"{citation_num}. [{label}]({url})\n"
                else:
                    # å¼•ç”¨ç¼–å·åœ¨æŠ¥å‘Šä¸­å­˜åœ¨ï¼Œä½†æ²¡æœ‰å¯¹åº”çš„æ¥æº
                    logger.warning(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å¼•ç”¨ç¼–å· [{citation_num}] æ²¡æœ‰å¯¹åº”çš„æ¥æº")
                    enhanced_content += f"{citation_num}. æ¥æºæœªæ‰¾åˆ°\n"
            
            logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å·²æ·»åŠ  {len(sorted_citations)} ä¸ªå‚è€ƒæ¥æº")
        else:
            logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘æŠ¥å‘Šå·²åŒ…å«å‚è€ƒæ¥æºéƒ¨åˆ†ï¼Œè·³è¿‡æ·»åŠ ")
    else:
        logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘æŠ¥å‘Šä¸­æœªæ‰¾åˆ°å¼•ç”¨ç¼–å·ï¼Œè·³è¿‡æ·»åŠ å‚è€ƒæ¥æºåˆ—è¡¨")
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æœ€ç»ˆç­”æ¡ˆåŒ…å« {len(unique_sources)} ä¸ªæ•°æ®æº")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æœ€ç»ˆå†…å®¹é•¿åº¦: {len(enhanced_content)} å­—ç¬¦")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")

    return {
        "messages": [AIMessage(content=enhanced_content)],
        "sources_gathered": unique_sources,
    }


_builder = StateGraph(OverallState)
_builder.add_node("generate_research_plan", generate_research_plan)
_builder.add_node("generate_query", generate_query)
_builder.add_node("web_research", web_research)
_builder.add_node("reflection", reflection)
# æ·»åŠ è´¨é‡å¢å¼ºèŠ‚ç‚¹
_builder.add_node("assess_content_quality", assess_content_quality)
_builder.add_node("verify_facts", verify_facts)
_builder.add_node("assess_relevance", assess_relevance)
_builder.add_node("optimize_summary", optimize_summary)
_builder.add_node("generate_verification_report", generate_verification_report)
_builder.add_node("finalize_answer", finalize_answer)

# è®¾ç½®å…¥å£ç‚¹
_builder.add_edge(START, "generate_research_plan")
_builder.add_edge("generate_research_plan", "generate_query")
_builder.add_conditional_edges("generate_query", continue_to_web_research, ["web_research"])
_builder.add_edge("web_research", "reflection")
_builder.add_conditional_edges("reflection", evaluate_research, ["generate_query", "assess_content_quality"])

# è´¨é‡å¢å¼ºæµç¨‹
_builder.add_edge("assess_content_quality", "verify_facts")
_builder.add_edge("verify_facts", "assess_relevance")
_builder.add_edge("assess_relevance", "optimize_summary")
_builder.add_edge("optimize_summary", "generate_verification_report")
_builder.add_edge("generate_verification_report", "finalize_answer")

# ç»“æŸèŠ‚ç‚¹
_builder.add_edge("finalize_answer", END)

graph = _builder.compile(name="enhanced-pro-search-engine")

logger.info("ã€å›¾æ„å»ºå®Œæˆã€‘å¢å¼ºå‹ Pro Search Engine å·²ç¼–è¯‘å®Œæˆ (å·²åŠ å…¥ç ”ç©¶æ–¹æ¡ˆæ­¥éª¤)")


