from typing import Any, Dict, List, Optional, TypedDict, Callable, TypeVar

from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_openai import ChatOpenAI
from typing_extensions import Annotated
from langgraph.graph import add_messages
import operator
import requests
import logging

from app.core.config import settings

T = TypeVar('T')
from .deepsearch_prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
    content_quality_instructions,
    fact_verification_instructions,
    relevance_assessment_instructions,
    summary_optimization_instructions,
    research_plan_instructions,
)
from .deepsearch_utils import (
    get_citations_from_bocha,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
    format_bocha_search_results,
)
from .deepsearch_types import (
    SearchQueryList, 
    Reflection,
    ContentQualityAssessment,
    FactVerification,
    RelevanceAssessment,
    SummaryOptimization,
    ResearchPlan,
)


logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šè·Ÿè¸ªæ˜¯å¦å·²é™çº§åˆ° Qwen3Max
_gemini_degraded = False


def reset_degradation_status():
    """é‡ç½®é™çº§çŠ¶æ€ï¼ˆç”¨äºæµ‹è¯•æˆ–æ–°è¯·æ±‚å¼€å§‹æ—¶ï¼‰."""
    global _gemini_degraded
    _gemini_degraded = False
    logger.info("ã€é™çº§çŠ¶æ€ã€‘å·²é‡ç½®ï¼Œå°†é‡æ–°å°è¯• Gemini")


def get_qwen_base_url() -> str:
    """è·å– Qwen3Max API base URL."""
    base_url = settings.DASHSCOPE_BASE_URL
    if not base_url:
        raise ValueError("DASHSCOPE_BASE_URL is not set")
    # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
    base_url = base_url.rstrip('/')
    # ç¡®ä¿ä»¥ /v1 ç»“å°¾
    if not base_url.endswith('/v1'):
        base_url = f"{base_url}/v1"
    logger.debug(f"Qwen3Max API base URL: {base_url}")
    return base_url


def get_gemini_base_url() -> str:
    """è·å– Gemini API base URLï¼Œç¡®ä¿ä»¥ /v1 ç»“å°¾"""
    base_url = settings.GEMINI_API_URL
    if not base_url:
        raise ValueError("GEMINI_API_URL is not set")
    # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
    base_url = base_url.rstrip('/')
    # ç¡®ä¿ä»¥ /v1 ç»“å°¾
    if not base_url.endswith('/v1'):
        base_url = f"{base_url}/v1"
    logger.debug(f"Gemini API base URL: {base_url}")
    return base_url


if not settings.GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY is not set")


def create_llm_with_fallback(
    model: str,
    temperature: float,
    use_gemini: bool = True,
    max_retries: int = 2
) -> ChatOpenAI:
    """
    åˆ›å»º LLM å®ä¾‹ï¼Œæ”¯æŒ Gemini å’Œ Qwen3Max åˆ‡æ¢.
    
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        use_gemini: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ Geminiï¼ˆTrueï¼‰æˆ– Qwen3Maxï¼ˆFalseï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        ChatOpenAI: LLM å®ä¾‹
    """
    if use_gemini:
        base_url = get_gemini_base_url()
        api_key = settings.GEMINI_API_KEY
        logger.debug(f"åˆ›å»º Gemini LLM å®ä¾‹: {model}")
    else:
        base_url = get_qwen_base_url()
        api_key = settings.DASHSCOPE_API_KEY
        if not api_key:
            raise ValueError("DASHSCOPE_API_KEY is not set")
        logger.debug(f"åˆ›å»º Qwen3Max LLM å®ä¾‹: {model}")
    
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        max_retries=max_retries,
        api_key=api_key,
        base_url=base_url,
        timeout=settings.API_TIMEOUT,
    )


def invoke_llm_with_fallback(
    invoke_func: Callable[[ChatOpenAI], T],
    node_name: str,
    gemini_model: str,
    temperature: float = 0.5,
    qwen_model: str = None,
    structured_output_type: Any = None,
    **llm_kwargs
) -> T:
    """
    è°ƒç”¨ LLMï¼Œæ”¯æŒ Gemini å¤±è´¥åè‡ªåŠ¨åˆ‡æ¢åˆ° Qwen3Max.
    
    å¦‚æœå·²ç»é™çº§åˆ° Qwen3Maxï¼Œç›´æ¥ä½¿ç”¨ Qwen3Maxï¼Œä¸å†å°è¯• Geminiã€‚
    å¦åˆ™å…ˆå°è¯•ä½¿ç”¨ Geminiï¼ˆé‡è¯•2æ¬¡ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™åˆ‡æ¢åˆ° Qwen3Maxï¼Œå¹¶è®¾ç½®å…¨å±€é™çº§æ ‡å¿—ã€‚
    
    Args:
        invoke_func: è°ƒç”¨å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªå‚æ•°ï¼ˆllm å®ä¾‹ï¼‰å¹¶è¿”å›ç»“æœ
        node_name: èŠ‚ç‚¹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        gemini_model: Gemini æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        qwen_model: Qwen3Max æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ DASHSCOPE_CHAT_MODELï¼‰
        structured_output_type: ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼ˆå¦‚æœæä¾›ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ with_structured_outputï¼‰
        **llm_kwargs: ä¼ é€’ç»™ ChatOpenAI çš„å…¶ä»–å‚æ•°
        
    Returns:
        è°ƒç”¨ç»“æœ
        
    Raises:
        Exception: å¦‚æœä¸¤ç§æ¨¡å‹éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
    """
    global _gemini_degraded
    
    if qwen_model is None:
        qwen_model = settings.DASHSCOPE_CHAT_MODEL
    
    # å¦‚æœå·²ç»é™çº§ï¼Œç›´æ¥ä½¿ç”¨ Qwen3Max
    if _gemini_degraded:
        logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘å·²é™çº§ï¼Œç›´æ¥ä½¿ç”¨ Qwen3Max ({qwen_model})...")
        try:
            llm = create_llm_with_fallback(
                model=qwen_model,
                temperature=temperature,
                use_gemini=False,
                max_retries=2
            )
            
            # å¦‚æœæŒ‡å®šäº†ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼Œä½¿ç”¨ with_structured_output
            if structured_output_type is not None:
                llm = llm.with_structured_output(structured_output_type)
            
            result = invoke_func(llm)
            logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Qwen3Max è°ƒç”¨æˆåŠŸ")
            return result
        except Exception as e:
            logger.error(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Qwen3Max è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            raise e
    
    # å°è¯•ä½¿ç”¨ Geminiï¼ˆé‡è¯•2æ¬¡ï¼‰
    last_error = None
    for attempt in range(3):  # ç¬¬ä¸€æ¬¡ + 2æ¬¡é‡è¯• = æ€»å…±3æ¬¡å°è¯•
        try:
            if attempt == 0:
                logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘å°è¯•ä½¿ç”¨ Gemini ({gemini_model})...")
            else:
                logger.warning(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini ç¬¬ {attempt} æ¬¡é‡è¯•...")
            
            llm = create_llm_with_fallback(
                model=gemini_model,
                temperature=temperature,
                use_gemini=True,
                max_retries=0  # åœ¨åŒ…è£…å‡½æ•°ä¸­æ‰‹åŠ¨å¤„ç†é‡è¯•
            )
            
            # å¦‚æœæŒ‡å®šäº†ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼Œä½¿ç”¨ with_structured_output
            if structured_output_type is not None:
                llm = llm.with_structured_output(structured_output_type)
            
            result = invoke_func(llm)
            logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini è°ƒç”¨æˆåŠŸ")
            return result
            
        except Exception as e:
            last_error = e
            logger.warning(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/3): {str(e)}")
            if attempt < 2:  # è¿˜æœ‰é‡è¯•æœºä¼š
                continue
            else:
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œåˆ‡æ¢åˆ° Qwen3Max å¹¶è®¾ç½®é™çº§æ ‡å¿—
                logger.warning(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Gemini é‡è¯•2æ¬¡åä»å¤±è´¥ï¼Œåˆ‡æ¢åˆ° Qwen3Max ({qwen_model})...")
                logger.warning(f"ã€é™çº§çŠ¶æ€ã€‘å·²è®¾ç½®é™çº§æ ‡å¿—ï¼Œåç»­æ‰€æœ‰è°ƒç”¨å°†ç›´æ¥ä½¿ç”¨ Qwen3Max")
                _gemini_degraded = True
                break
    
    # åˆ‡æ¢åˆ° Qwen3Max
    try:
        llm = create_llm_with_fallback(
            model=qwen_model,
            temperature=temperature,
            use_gemini=False,
            max_retries=2
        )
        
        # å¦‚æœæŒ‡å®šäº†ç»“æ„åŒ–è¾“å‡ºç±»å‹ï¼Œä½¿ç”¨ with_structured_output
        if structured_output_type is not None:
            llm = llm.with_structured_output(structured_output_type)
        
        result = invoke_func(llm)
        logger.info(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Qwen3Max è°ƒç”¨æˆåŠŸ")
        return result
    except Exception as e:
        logger.error(f"ã€èŠ‚ç‚¹: {node_name}ã€‘Qwen3Max è°ƒç”¨ä¹Ÿå¤±è´¥: {str(e)}", exc_info=True)
        # å¦‚æœ Qwen3Max ä¹Ÿå¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
        raise e


class OverallState(TypedDict, total=False):
    messages: Annotated[List, add_messages]
    research_plan: Optional[ResearchPlan]
    search_query: Annotated[List, operator.add]
    web_research_result: Annotated[List, operator.add]
    sources_gathered: Annotated[List, operator.add]
    all_sources_gathered: Annotated[List, operator.add]  # æ‰€æœ‰æœç´¢åˆ°çš„èµ„æºï¼ˆåŒ…æ‹¬æœªè¢«å¼•ç”¨çš„ï¼‰
    initial_search_query_count: int
    max_research_loops: int
    research_loop_count: int
    reasoning_model: str
    # è´¨é‡å¢å¼ºç›¸å…³å­—æ®µ
    content_quality: Dict[str, Any]
    fact_verification: Dict[str, Any]
    relevance_assessment: Dict[str, Any]
    summary_optimization: Dict[str, Any]
    verification_report: str
    final_confidence_score: float


class ReflectionState(TypedDict):
    is_sufficient: bool
    knowledge_gap: str
    follow_up_queries: Annotated[List, operator.add]
    research_loop_count: int
    number_of_ran_queries: int
    max_research_loops: int  # æ·»åŠ æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•°å­—æ®µ


class Query(TypedDict):
    query: str
    rationale: str


class QueryGenerationState(TypedDict):
    search_query: List[str]  # å®é™…ä¸Šæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸æ˜¯ Query å¯¹è±¡åˆ—è¡¨


class WebSearchState(TypedDict):
    search_query: str
    id: str


def generate_research_plan(state: OverallState, config: RunnableConfig) -> OverallState:
    """
    ç”Ÿæˆç ”ç©¶æ–¹æ¡ˆèŠ‚ç‚¹ã€‚
    """
    logger.info("ã€èŠ‚ç‚¹: generate_research_planã€‘å¼€å§‹ç”Ÿæˆç ”ç©¶æ–¹æ¡ˆ...")
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    research_topic = get_research_topic(state["messages"])
    logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç ”ç©¶ä¸»é¢˜: {research_topic[:200]}...")
    
    formatted_prompt = research_plan_instructions.format(
        research_topic=research_topic
    )
    
    logger.info("ã€èŠ‚ç‚¹: generate_research_planã€‘è°ƒç”¨ LLM ç”Ÿæˆæ–¹æ¡ˆ...")
    try:
        plan = invoke_llm_with_fallback(
            invoke_func=lambda llm: llm.invoke(formatted_prompt),
            node_name="generate_research_plan",
            gemini_model=reasoning_model,
            temperature=0.3,  # ä»0.5é™ä½åˆ°0.3ï¼Œè®©è¾“å‡ºæ›´ä¸¥è°¨è¯¦ç»†
            structured_output_type=ResearchPlan
        )
        logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç ”ç©¶æ–¹æ¡ˆç”Ÿæˆå®Œæ¯•ï¼ŒåŒ…å« {len(plan.sub_topics)} ä¸ªå­ä¸»é¢˜")
        logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç ”ç©¶é—®é¢˜æ€»æ•°: {len(plan.research_questions)}")
        for idx, sub_topic in enumerate(plan.sub_topics, 1):
            logger.info(f"ã€èŠ‚ç‚¹: generate_research_planã€‘  å­ä¸»é¢˜ {idx}: {sub_topic}")
        return {"research_plan": plan}
    except Exception as e:
        logger.error(f"ã€èŠ‚ç‚¹: generate_research_planã€‘ç”Ÿæˆæ–¹æ¡ˆå¤±è´¥: {e}", exc_info=True)
        return {"research_plan": None}


def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    logger.info("ã€èŠ‚ç‚¹: generate_queryã€‘å¼€å§‹ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
    initial_count = state.get("initial_search_query_count")
    if initial_count is None:
        initial_count = 3
        state["initial_search_query_count"] = initial_count
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡: {initial_count}")
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")

    research_topic = get_research_topic(state["messages"])
    research_plan = state.get("research_plan")
    
    # å°†æ–¹æ¡ˆæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    plan_str = "æ— ç‰¹å®šæ–¹æ¡ˆï¼Œè¯·ç›´æ¥åˆ†æç ”ç©¶ä¸»é¢˜ã€‚"
    if research_plan and research_plan.sub_topics:
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘åŸºäºæ–¹æ¡ˆ '{research_plan.research_topic}' ç”ŸæˆæŸ¥è¯¢")
        plan_str = f"ä¸»é¢˜: {research_plan.research_topic}\n\nå…³é”®å­ä¸»é¢˜å’Œç ”ç©¶é—®é¢˜:\n"
        
        # æŒ‰å­ä¸»é¢˜åˆ†ç»„ç ”ç©¶é—®é¢˜
        for i, sub_topic in enumerate(research_plan.sub_topics, 1):
            plan_str += f"\n{i}. {sub_topic}\n"
            plan_str += "   ç ”ç©¶é—®é¢˜:\n"
            # æ‰¾å‡ºå±äºå½“å‰å­ä¸»é¢˜çš„ç ”ç©¶é—®é¢˜
            topic_questions = [q for q in research_plan.research_questions if q.startswith(f"{sub_topic}ï¼š")]
            if not topic_questions:
                # å¦‚æœæ²¡æœ‰ä¸¥æ ¼åŒ¹é…çš„ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                topic_questions = [q for q in research_plan.research_questions if sub_topic in q]
            for j, question in enumerate(topic_questions, 1):
                # ç§»é™¤ã€Œå­ä¸»é¢˜ï¼šã€å‰ç¼€ï¼Œåªæ˜¾ç¤ºé—®é¢˜æœ¬èº«
                question_text = question.split("ï¼š", 1)[-1] if "ï¼š" in question else question
                plan_str += f"   {i}.{j}. {question_text}\n"
        
        plan_str += f"\nç†ç”±: {research_plan.rationale}"
    else:
        logger.warning("ã€èŠ‚ç‚¹: generate_queryã€‘æœªæ‰¾åˆ°ç ”ç©¶æ–¹æ¡ˆï¼Œå°†ä»…åŸºäºä¸»é¢˜ç”ŸæˆæŸ¥è¯¢")
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ç ”ç©¶ä¸»é¢˜: {research_topic[:200]}...")
    
    formatted_prompt = query_writer_instructions.format(
        current_date=get_current_date(),
        research_topic=research_topic,
        research_plan=plan_str,
        number_queries=initial_count,
    )
    
    logger.info("ã€èŠ‚ç‚¹: generate_queryã€‘è°ƒç”¨ LLM (åŸºäºæ–¹æ¡ˆ) ç”ŸæˆæŸ¥è¯¢...")
    result = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(formatted_prompt),
        node_name="generate_query",
        gemini_model=reasoning_model,
        temperature=1.0,
        structured_output_type=SearchQueryList
    )
    
    query_count = len(result.query) if result.query else 0
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘æˆåŠŸç”Ÿæˆ {query_count} ä¸ªæœç´¢æŸ¥è¯¢")
    for idx, query_item in enumerate(result.query[:5], 1):  # åªè®°å½•å‰5ä¸ª
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘  æŸ¥è¯¢ {idx}: {query_item[:100]}...")
    
    return {"search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    query_count = len(state["search_query"])
    logger.info(f"ã€èŠ‚ç‚¹: continue_to_web_researchã€‘å‡†å¤‡åˆ†å‘ {query_count} ä¸ªæœç´¢ä»»åŠ¡åˆ° web_research èŠ‚ç‚¹")
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


def bocha_web_search(query: str, count: int = 10) -> Dict[str, Any]:
    """
    ä½¿ç”¨åšæŸ¥æœç´¢ API è¿›è¡Œç½‘é¡µæœç´¢ã€‚

    å‚æ•°:
    - query: æœç´¢å…³é”®è¯
    - count: è¿”å›çš„æœç´¢ç»“æœæ•°é‡

    è¿”å›:
    - åŒ…å«æœç´¢ç»“æœå’Œæ ¼å¼åŒ–æ–‡æœ¬çš„å­—å…¸
    """
    if not settings.BOCHA_API_KEY:
        raise ValueError("BOCHA_API_KEY is not set in environment variables")

    url = 'https://api.bochaai.com/v1/web-search'
    headers = {
        'Authorization': f'Bearer {settings.BOCHA_API_KEY}',
        'Content-Type': 'application/json'
    }
    data = {
        "query": query,
        "freshness": "noLimit",  # æœç´¢çš„æ—¶é—´èŒƒå›´
        "summary": True,  # æ˜¯å¦è¿”å›é•¿æ–‡æœ¬æ‘˜è¦
        "count": count
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            json_response = response.json()
            if json_response.get("code") != 200 or not json_response.get("data"):
                error_msg = json_response.get("msg", "æœªçŸ¥é”™è¯¯")
                logger.error(f"åšæŸ¥æœç´¢APIè¯·æ±‚å¤±è´¥: {error_msg}")
                return {
                    "webpages": [],
                    "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒåŸå› æ˜¯: {error_msg}"
                }
            
            webpages = json_response.get("data", {}).get("webPages", {}).get("value", [])
            if not webpages:
                logger.warning(f"åšæŸ¥æœç´¢APIè¿”å›ç©ºç»“æœï¼ŒæŸ¥è¯¢: {query[:100]}...")
                return {
                    "webpages": [],
                    "formatted_text": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
                }
            
            logger.info(f"åšæŸ¥æœç´¢APIæˆåŠŸè¿”å› {len(webpages)} ä¸ªç»“æœï¼ŒæŸ¥è¯¢: {query[:100]}...")
            formatted_text = format_bocha_search_results(webpages)
            return {
                "webpages": webpages,
                "formatted_text": formatted_text
            }
        else:
            error_msg = f"çŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {response.text}"
            logger.error(f"åšæŸ¥æœç´¢APIè¯·æ±‚å¤±è´¥: {error_msg}")
            return {
                "webpages": [],
                "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼Œ{error_msg}"
            }
    except Exception as e:
        logger.error(f"åšæŸ¥æœç´¢APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return {
            "webpages": [],
            "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒåŸå› æ˜¯ï¼š{str(e)}"
        }


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """
    ä½¿ç”¨åšæŸ¥æœç´¢ API è¿›è¡Œç½‘é¡µç ”ç©¶ã€‚
    """
    search_query = state["search_query"]
    search_id = state["id"]
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡ ID={search_id}")
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢æŸ¥è¯¢: {search_query[:200]}...")
    
    # è°ƒç”¨åšæŸ¥æœç´¢ API
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘è°ƒç”¨åšæŸ¥æœç´¢ API...")
    search_result = bocha_web_search(query=search_query, count=10)
    
    webpages = search_result.get("webpages", [])
    formatted_text = search_result.get("formatted_text", "")
    
    webpage_count = len(webpages) if webpages else 0
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {webpage_count} ä¸ªç½‘é¡µç»“æœ")
    
    if webpage_count > 0:
        logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å‰3ä¸ªç»“æœæ ‡é¢˜:")
        for idx, page in enumerate(webpages[:3], 1):
            logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘  {idx}. {page.get('name', 'N/A')[:100]}")
    
    # ä½¿ç”¨ Gemini å¯¹æœç´¢ç»“æœè¿›è¡Œæ€»ç»“å’Œæ•´ç†
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹ä½¿ç”¨ LLM æ€»ç»“æœç´¢ç»“æœ...")
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=search_query,
    )

    # å°†æœç´¢ç»“æœæ·»åŠ åˆ°æç¤ºè¯ä¸­ï¼Œå¹¶æç¤º LLM ä½¿ç”¨å¼•ç”¨ç¼–å·
    search_context = (
        f"\n\næœç´¢æŸ¥è¯¢: {search_query}\n"
        f"æœç´¢ç»“æœï¼ˆè¯·åœ¨ä½ çš„å›ç­”ä¸­ä½¿ç”¨å¼•ç”¨ç¼–å· [1], [2] ç­‰æ¥å¼•ç”¨è¿™äº›æ¥æºï¼‰:\n{formatted_text}"
    )
    full_prompt = formatted_prompt + search_context

    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘è°ƒç”¨ LLM APIï¼Œæç¤ºè¯é•¿åº¦: {len(full_prompt)} å­—ç¬¦")
    llm_response = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(full_prompt),
        node_name="web_research",
        gemini_model=settings.GEMINI_MODEL,
        temperature=0
    )
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘LLM æ€»ç»“å®Œæˆï¼Œå“åº”é•¿åº¦: {len(llm_response.content)} å­—ç¬¦")
    
    # å¤„ç†å¼•ç”¨å’Œæ¥æº
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹å¤„ç†å¼•ç”¨å’Œæ¥æº...")
    resolved_urls = resolve_urls(webpages, search_id)
    citations = get_citations_from_bocha(webpages, resolved_urls, llm_response.content)
    modified_text = insert_citation_markers(llm_response.content, citations)
    sources_gathered = [item for citation in citations for item in citation["segments"]]
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(citations)} ä¸ªå¼•ç”¨ï¼Œ{len(sources_gathered)} ä¸ªæ•°æ®æºç‰‡æ®µ")
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢ä»»åŠ¡ ID={search_id} å®Œæˆ")

    return {
        "sources_gathered": sources_gathered,
        "all_sources_gathered": sources_gathered,  # ä¿å­˜æ‰€æœ‰æœç´¢åˆ°çš„èµ„æº
        "search_query": [search_query],
        "web_research_result": [modified_text],
    }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    loop_count = state["research_loop_count"]
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘å¼€å§‹åæ€ï¼Œç ”ç©¶å¾ªç¯æ¬¡æ•°: {loop_count}")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    web_research_results = state.get("web_research_result", [])
    search_queries = state.get("search_query", [])
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘å½“å‰å·²æœ‰ {len(web_research_results)} ä¸ªæœç´¢ç»“æœï¼Œ{len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")

    formatted_prompt = reflection_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        loop_count=loop_count,
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    
    logger.info("ã€èŠ‚ç‚¹: reflectionã€‘è°ƒç”¨ LLM è¿›è¡Œåæ€è¯„ä¼°...")
    result = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(formatted_prompt),
        node_name="reflection",
        gemini_model=reasoning_model,
        temperature=1.0,
        structured_output_type=Reflection
    )

    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘=== ä¿¡æ¯å……è¶³æ€§è¯„ä¼°ç»“æœ ===")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  å½“å‰å¾ªç¯: ç¬¬ {loop_count} è½®")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  ä¿¡æ¯æ˜¯å¦å……è¶³: {result.is_sufficient}")
    
    if result.is_sufficient:
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  âœ… è¯„ä¼°ç»“æœ: ä¿¡æ¯å·²å……è¶³ï¼Œå¯ä»¥å¼€å§‹ç”ŸæˆæŠ¥å‘Š")
    else:
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  âš ï¸  è¯„ä¼°ç»“æœ: ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç»§ç»­ç ”ç©¶")
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  çŸ¥è¯†ç¼ºå£: {result.knowledge_gap[:200] if result.knowledge_gap else 'N/A'}...")
        follow_up_count = len(result.follow_up_queries) if result.follow_up_queries else 0
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  åç»­æŸ¥è¯¢æ•°é‡: {follow_up_count}")
        if follow_up_count > 0:
            for idx, query in enumerate(result.follow_up_queries[:3], 1):  # åªè®°å½•å‰3ä¸ª
                logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘    åç»­æŸ¥è¯¢ {idx}: {query[:100]}...")

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
        "max_research_loops": state.get("max_research_loops", 5),  # ä¼ é€’æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œé»˜è®¤5
    }


def evaluate_research(state: ReflectionState, config: RunnableConfig) -> OverallState:
    max_research_loops = state.get("max_research_loops", 5)  # é»˜è®¤5æ¬¡å¾ªç¯
    loop_count = state["research_loop_count"]
    is_sufficient = state["is_sufficient"]
    
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘=== ç ”ç©¶çŠ¶æ€å†³ç­– ===")
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘  å½“å‰å¾ªç¯æ¬¡æ•°: {loop_count}/{max_research_loops}")
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘  ä¿¡æ¯æ˜¯å¦å……è¶³: {is_sufficient}")
    
    if state["is_sufficient"]:
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘âœ… å†³ç­–: ä¿¡æ¯å·²å……è¶³ï¼Œç»“æŸè°ƒæŸ¥å¾ªç¯")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: å¼€å§‹è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹")
        return "assess_content_quality"
    elif state["research_loop_count"] >= max_research_loops:
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘âš ï¸  å†³ç­–: å·²è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° ({max_research_loops})ï¼Œå¼ºåˆ¶ç»“æŸ")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: åŸºäºç°æœ‰ä¿¡æ¯ç”ŸæˆæŠ¥å‘Š")
        return "assess_content_quality"
    else:
        follow_up_queries = state.get("follow_up_queries", [])
        follow_up_count = len(follow_up_queries)
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘ğŸ”„ å†³ç­–: ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­ç¬¬ {loop_count + 1} è½®è°ƒæŸ¥")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: åˆ†å‘ {follow_up_count} ä¸ªåç»­æŸ¥è¯¢åˆ° web_research èŠ‚ç‚¹")
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def assess_content_quality(state: OverallState, config: RunnableConfig):
    """å†…å®¹è´¨é‡è¯„ä¼°èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘å¼€å§‹å†…å®¹è´¨é‡è¯„ä¼°")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    formatted_prompt = content_quality_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    result = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(formatted_prompt),
        node_name="assess_content_quality",
        gemini_model=reasoning_model,
        temperature=0.3,
        structured_output_type=ContentQualityAssessment
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘è´¨é‡è¯„åˆ†: {result.quality_score}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘å†…å®¹ç©ºç™½æ•°é‡: {len(result.content_gaps)}")
    
    return {
        "content_quality": {
            "quality_score": result.quality_score,
            "reliability_assessment": result.reliability_assessment,
            "content_gaps": result.content_gaps,
            "improvement_suggestions": result.improvement_suggestions
        }
    }


def verify_facts(state: OverallState, config: RunnableConfig):
    """äº‹å®éªŒè¯èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘å¼€å§‹äº‹å®éªŒè¯")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = fact_verification_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    def invoke_with_method(llm: ChatOpenAI):
        """è°ƒç”¨å¸¦ method å‚æ•°çš„ structured_output."""
        structured_llm = llm.with_structured_output(
            FactVerification,
            method="json_schema",
            include_raw=False
        )
        return structured_llm.invoke(formatted_prompt)
    
    result = invoke_llm_with_fallback(
        invoke_func=invoke_with_method,
        node_name="verify_facts",
        gemini_model=reasoning_model,
        temperature=0.1
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘éªŒè¯ç½®ä¿¡åº¦: {result.confidence_score}")
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘å·²éªŒè¯äº‹å®æ•°é‡: {len(result.verified_facts_text)}")
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘äº‰è®®å£°æ˜æ•°é‡: {len(result.disputed_claims_text)}")
    
    # å°†æ‰å¹³åŒ–çš„åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    verified_facts_dicts = [
        {"fact": fact, "source": source} 
        for fact, source in zip(result.verified_facts_text, result.verified_facts_sources)
    ]
    disputed_claims_dicts = [
        {"claim": claim, "reason": reason} 
        for claim, reason in zip(result.disputed_claims_text, result.disputed_claims_reasons)
    ]
    
    return {
        "fact_verification": {
            "verified_facts": verified_facts_dicts,
            "disputed_claims": disputed_claims_dicts,
            "verification_sources": result.verification_sources,
            "confidence_score": result.confidence_score
        }
    }


def assess_relevance(state: OverallState, config: RunnableConfig):
    """ç›¸å…³æ€§è¯„ä¼°èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘å¼€å§‹ç›¸å…³æ€§è¯„ä¼°")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    formatted_prompt = relevance_assessment_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    result = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(formatted_prompt),
        node_name="assess_relevance",
        gemini_model=reasoning_model,
        temperature=0.2,
        structured_output_type=RelevanceAssessment
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ç›¸å…³æ€§è¯„åˆ†: {result.relevance_score}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘è¦†ç›–å…³é”®ä¸»é¢˜æ•°é‡: {len(result.key_topics_covered)}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ç¼ºå¤±ä¸»é¢˜æ•°é‡: {len(result.missing_topics)}")
    
    return {
        "relevance_assessment": {
            "relevance_score": result.relevance_score,
            "key_topics_covered": result.key_topics_covered,
            "missing_topics": result.missing_topics,
            "content_alignment": result.content_alignment
        }
    }


def optimize_summary(state: OverallState, config: RunnableConfig):
    """æ‘˜è¦ä¼˜åŒ–èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å¼€å§‹æ‘˜è¦ä¼˜åŒ–")
    
    # è·å–åŸå§‹æ‘˜è¦
    original_summary = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = summary_optimization_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        original_summary=original_summary,
        quality_assessment=str(state.get("content_quality", {})),
        fact_verification=str(state.get("fact_verification", {})),
        relevance_assessment=str(state.get("relevance_assessment", {}))
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    result = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(formatted_prompt),
        node_name="optimize_summary",
        gemini_model=reasoning_model,
        temperature=0.3,
        structured_output_type=SummaryOptimization
    )
    
    # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†
    quality_score = state.get("content_quality", {}).get("quality_score", 0.5)
    fact_confidence = state.get("fact_verification", {}).get("confidence_score", 0.5)
    relevance_score = state.get("relevance_assessment", {}).get("relevance_score", 0.5)
    final_confidence = (quality_score + fact_confidence + relevance_score) / 3
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å…³é”®æ´å¯Ÿæ•°é‡: {len(result.key_insights)}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å¯è¡Œå»ºè®®æ•°é‡: {len(result.actionable_items)}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘ç½®ä¿¡åº¦ç­‰çº§: {result.confidence_level}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†: {final_confidence:.3f}")
    
    return {
        "summary_optimization": {
            "key_insights": result.key_insights,
            "actionable_items": result.actionable_items,
            "confidence_level": result.confidence_level
        },
        "final_confidence_score": final_confidence
    }


def generate_verification_report(state: OverallState, config: RunnableConfig):
    """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘ŠèŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: generate_verification_reportã€‘å¼€å§‹ç”ŸæˆéªŒè¯æŠ¥å‘Š")
    
    # ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š
    quality_data = state.get("content_quality", {})
    fact_data = state.get("fact_verification", {})
    relevance_data = state.get("relevance_assessment", {})
    optimization_data = state.get("summary_optimization", {})
    
    report = f"""
# ç ”ç©¶è´¨é‡éªŒè¯æŠ¥å‘Š

## å†…å®¹è´¨é‡è¯„ä¼°
- **è´¨é‡è¯„åˆ†**: {quality_data.get('quality_score', 'N/A'):.2f}/1.0
- **å¯é æ€§è¯„ä¼°**: {quality_data.get('reliability_assessment', 'N/A')}
- **å†…å®¹ç©ºç™½**: {', '.join(quality_data.get('content_gaps', [])) if quality_data.get('content_gaps') else 'æ— æ˜æ˜¾ç©ºç™½'}
- **æ”¹è¿›å»ºè®®**: {', '.join(quality_data.get('improvement_suggestions', [])) if quality_data.get('improvement_suggestions') else 'æ— ç‰¹åˆ«å»ºè®®'}

## äº‹å®éªŒè¯ç»“æœ
- **éªŒè¯ç½®ä¿¡åº¦**: {fact_data.get('confidence_score', 'N/A'):.2f}/1.0
- **å·²éªŒè¯äº‹å®æ•°é‡**: {len(fact_data.get('verified_facts', []))}
- **äº‰è®®å£°æ˜æ•°é‡**: {len(fact_data.get('disputed_claims', []))}
- **éªŒè¯æ¥æº**: {', '.join(fact_data.get('verification_sources', [])) if fact_data.get('verification_sources') else 'å¤šä¸ªæ¥æº'}

## ç›¸å…³æ€§è¯„ä¼°
- **ç›¸å…³æ€§è¯„åˆ†**: {relevance_data.get('relevance_score', 'N/A'):.2f}/1.0
- **å·²è¦†ç›–å…³é”®ä¸»é¢˜**: {', '.join(relevance_data.get('key_topics_covered', [])) if relevance_data.get('key_topics_covered') else 'N/A'}
- **ç¼ºå¤±ä¸»é¢˜**: {', '.join(relevance_data.get('missing_topics', [])) if relevance_data.get('missing_topics') else 'æ— æ˜æ˜¾ç¼ºå¤±'}
- **å†…å®¹ä¸€è‡´æ€§**: {relevance_data.get('content_alignment', 'N/A')}

## æ‘˜è¦ä¼˜åŒ–ç»“æœ
- **ç½®ä¿¡åº¦ç­‰çº§**: {optimization_data.get('confidence_level', 'N/A')}
- **å…³é”®æ´å¯Ÿæ•°é‡**: {len(optimization_data.get('key_insights', []))}
- **å¯è¡Œå»ºè®®æ•°é‡**: {len(optimization_data.get('actionable_items', []))}

## ç»¼åˆè¯„ä¼°
- **æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†**: {state.get('final_confidence_score', 0):.3f}/1.0
"""
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_verification_reportã€‘éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    return {
        "verification_report": report
    }


def finalize_answer(state: OverallState, config: RunnableConfig):
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œè¿”å›é«˜åº¦å›´ç»•ç”¨æˆ·æé—®çš„è°ƒæŸ¥ç ”ç©¶æŠ¥å‘Šã€‚"""
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å¼€å§‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    web_research_results = state.get("web_research_result", [])
    sources_gathered = state.get("sources_gathered", [])
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æ±‡æ€» {len(web_research_results)} ä¸ªæœç´¢ç»“æœï¼Œ{len(sources_gathered)} ä¸ªæ•°æ®æº")

    # è·å–æ‰€æœ‰åŸå§‹ææ–™
    summaries = "\n---\n\n".join(web_research_results)
    
    # è·å–ä¸Šä¸€æ­¥çš„ç»“æ„åŒ–æ´å¯Ÿ
    optimization_data = state.get("summary_optimization", {})
    key_insights = optimization_data.get("key_insights", [])
    actionable_items = optimization_data.get("actionable_items", [])
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æ ¸å¿ƒæ´å¯Ÿæ•°é‡: {len(key_insights)}")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å¯è¡Œå»ºè®®æ•°é‡: {len(actionable_items)}")
    
    # æ„å»ºå¢å¼ºçš„æç¤ºè¯ï¼Œå°†ç»“æ„åŒ–æ´å¯Ÿæ³¨å…¥
    prompt_enhancement = ""
    if key_insights or actionable_items:
        prompt_enhancement = "\n\n---\n\n**ä»¥ä¸‹æ˜¯åŸºäºç ”ç©¶ææ–™æç‚¼å‡ºçš„æ ¸å¿ƒæ´å¯Ÿå’Œå»ºè®®ï¼Œè¯·å°†å®ƒä»¬ä½œä¸ºæŠ¥å‘Šçš„é‡ç‚¹ï¼Œåœ¨æŠ¥å‘Šä¸­è¯¦ç»†å±•å¼€è®ºè¿°ï¼š**\n\n"
        
        if key_insights:
            prompt_enhancement += "**æ ¸å¿ƒæ´å¯Ÿ (Key Insights):**\n"
            for i, insight in enumerate(key_insights, 1):
                prompt_enhancement += f"{i}. {insight}\n"
            prompt_enhancement += "\n"
        
        if actionable_items:
            prompt_enhancement += "**å¯è¡Œå»ºè®® (Actionable Items):**\n"
            for i, item in enumerate(actionable_items, 1):
                prompt_enhancement += f"{i}. {item}\n"
    
    # ä½¿ç”¨ answer_instructions æ¥æ’°å†™æŠ¥å‘Š
    formatted_prompt = answer_instructions.format(
        current_date=get_current_date(),
        research_topic=get_research_topic(state["messages"]),
        summaries=summaries + prompt_enhancement  # å°†æ´å¯Ÿæ³¨å…¥æç¤ºè¯
    )
    
    logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘è°ƒç”¨ LLM ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š...")
    
    # *** å…³é”®ï¼šè¿™é‡Œä¸ä½¿ç”¨ .with_structured_output()ï¼Œç›´æ¥ç”Ÿæˆçº¯æ–‡æœ¬æŠ¥å‘Š ***
    result = invoke_llm_with_fallback(
        invoke_func=lambda llm: llm.invoke(formatted_prompt),
        node_name="finalize_answer",
        gemini_model=reasoning_model,
        temperature=0.2
    )
    final_report = result.content  # è¿™å°±æ˜¯çº¯ Markdown æŠ¥å‘Š
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘LLM ç”Ÿæˆå®Œæˆï¼ŒæŠ¥å‘Šé•¿åº¦: {len(final_report)} å­—ç¬¦")
    
    # å¤„ç†æ•°æ®æºå¼•ç”¨
    logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘å¤„ç†æ•°æ®æºå¼•ç”¨...")
    unique_sources: List[Dict[str, Any]] = []
    enhanced_content = final_report
    
    for source in sources_gathered:
        if source["shortUrl"] in enhanced_content:
            enhanced_content = enhanced_content.replace(source["shortUrl"], source["value"])
            unique_sources.append(source)
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æœ€ç»ˆç­”æ¡ˆåŒ…å« {len(unique_sources)} ä¸ªè¢«å¼•ç”¨çš„æ•°æ®æº")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æœ€ç»ˆå†…å®¹é•¿åº¦: {len(enhanced_content)} å­—ç¬¦")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")

    return {
        "messages": [AIMessage(content=enhanced_content)],
        "sources_gathered": unique_sources,
    }


_builder = StateGraph(OverallState)
_builder.add_node("generate_research_plan", generate_research_plan)
_builder.add_node("generate_query", generate_query)
_builder.add_node("web_research", web_research)
_builder.add_node("reflection", reflection)
# æ·»åŠ è´¨é‡å¢å¼ºèŠ‚ç‚¹
_builder.add_node("assess_content_quality", assess_content_quality)
_builder.add_node("verify_facts", verify_facts)
_builder.add_node("assess_relevance", assess_relevance)
_builder.add_node("optimize_summary", optimize_summary)
_builder.add_node("generate_verification_report", generate_verification_report)
_builder.add_node("finalize_answer", finalize_answer)

# è®¾ç½®å…¥å£ç‚¹
_builder.add_edge(START, "generate_research_plan")
_builder.add_edge("generate_research_plan", "generate_query")
_builder.add_conditional_edges("generate_query", continue_to_web_research, ["web_research"])
_builder.add_edge("web_research", "reflection")
_builder.add_conditional_edges("reflection", evaluate_research, ["web_research", "assess_content_quality"])

# è´¨é‡å¢å¼ºæµç¨‹
_builder.add_edge("assess_content_quality", "verify_facts")
_builder.add_edge("verify_facts", "assess_relevance")
_builder.add_edge("assess_relevance", "optimize_summary")
_builder.add_edge("optimize_summary", "generate_verification_report")
_builder.add_edge("generate_verification_report", "finalize_answer")

# ç»“æŸèŠ‚ç‚¹
_builder.add_edge("finalize_answer", END)

graph = _builder.compile(name="enhanced-pro-search-engine")

logger.info("ã€å›¾æ„å»ºå®Œæˆã€‘å¢å¼ºå‹ Pro Search Engine å·²ç¼–è¯‘å®Œæˆ (å·²åŠ å…¥ç ”ç©¶æ–¹æ¡ˆæ­¥éª¤)")


