from typing import Any, Dict, List, Optional, TypedDict

from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_openai import ChatOpenAI
from typing_extensions import Annotated
from langgraph.graph import add_messages
import operator
import requests
import logging

from app.core.config import settings
from .deepsearch_prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
    content_quality_instructions,
    fact_verification_instructions,
    relevance_assessment_instructions,
    summary_optimization_instructions,
)
from .deepsearch_utils import (
    get_citations_from_bocha,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
    format_bocha_search_results,
)
from .deepsearch_types import (
    SearchQueryList, 
    Reflection,
    ContentQualityAssessment,
    FactVerification,
    RelevanceAssessment,
    SummaryOptimization,
)


logger = logging.getLogger(__name__)


def get_gemini_base_url() -> str:
    """è·å– Gemini API base URLï¼Œç¡®ä¿ä»¥ /v1 ç»“å°¾"""
    base_url = settings.GEMINI_API_URL
    if not base_url:
        raise ValueError("GEMINI_API_URL is not set")
    # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
    base_url = base_url.rstrip('/')
    # ç¡®ä¿ä»¥ /v1 ç»“å°¾
    if not base_url.endswith('/v1'):
        base_url = f"{base_url}/v1"
    logger.debug(f"Gemini API base URL: {base_url}")
    return base_url


if not settings.GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY is not set")


class OverallState(TypedDict, total=False):
    messages: Annotated[List, add_messages]
    search_query: Annotated[List, operator.add]
    web_research_result: Annotated[List, operator.add]
    sources_gathered: Annotated[List, operator.add]
    initial_search_query_count: int
    max_research_loops: int
    research_loop_count: int
    reasoning_model: str
    # è´¨é‡å¢å¼ºç›¸å…³å­—æ®µ
    content_quality: Dict[str, Any]
    fact_verification: Dict[str, Any]
    relevance_assessment: Dict[str, Any]
    summary_optimization: Dict[str, Any]
    quality_enhanced_summary: str
    verification_report: str
    final_confidence_score: float


class ReflectionState(TypedDict):
    is_sufficient: bool
    knowledge_gap: str
    follow_up_queries: Annotated[List, operator.add]
    research_loop_count: int
    number_of_ran_queries: int
    max_research_loops: int  # æ·»åŠ æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•°å­—æ®µ


class Query(TypedDict):
    query: str
    rationale: str


class QueryGenerationState(TypedDict):
    search_query: List[str]  # å®é™…ä¸Šæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸æ˜¯ Query å¯¹è±¡åˆ—è¡¨


class WebSearchState(TypedDict):
    search_query: str
    id: str


def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    logger.info("ã€èŠ‚ç‚¹: generate_queryã€‘å¼€å§‹ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
    initial_count = state.get("initial_search_query_count")
    if initial_count is None:
        initial_count = 3
        state["initial_search_query_count"] = initial_count
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡: {initial_count}")
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    gemini_base_url = get_gemini_base_url()
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘Gemini API URL: {gemini_base_url}")
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘Gemini API Key: {settings.GEMINI_API_KEY[:20]}...")

    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=1.0,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    structured_llm = llm.with_structured_output(SearchQueryList)

    research_topic = get_research_topic(state["messages"])
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘ç ”ç©¶ä¸»é¢˜: {research_topic[:200]}...")
    
    formatted_prompt = query_writer_instructions.format(
        current_date=get_current_date(),
        research_topic=research_topic,
        number_queries=initial_count,
    )
    
    logger.info("ã€èŠ‚ç‚¹: generate_queryã€‘è°ƒç”¨ LLM ç”ŸæˆæŸ¥è¯¢...")
    result = structured_llm.invoke(formatted_prompt)
    
    query_count = len(result.query) if result.query else 0
    logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘æˆåŠŸç”Ÿæˆ {query_count} ä¸ªæœç´¢æŸ¥è¯¢")
    for idx, query_item in enumerate(result.query[:5], 1):  # åªè®°å½•å‰5ä¸ª
        logger.info(f"ã€èŠ‚ç‚¹: generate_queryã€‘  æŸ¥è¯¢ {idx}: {query_item[:100]}...")
    
    return {"search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    query_count = len(state["search_query"])
    logger.info(f"ã€èŠ‚ç‚¹: continue_to_web_researchã€‘å‡†å¤‡åˆ†å‘ {query_count} ä¸ªæœç´¢ä»»åŠ¡åˆ° web_research èŠ‚ç‚¹")
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


def bocha_web_search(query: str, count: int = 10) -> Dict[str, Any]:
    """
    ä½¿ç”¨åšæŸ¥æœç´¢ API è¿›è¡Œç½‘é¡µæœç´¢ã€‚

    å‚æ•°:
    - query: æœç´¢å…³é”®è¯
    - count: è¿”å›çš„æœç´¢ç»“æœæ•°é‡

    è¿”å›:
    - åŒ…å«æœç´¢ç»“æœå’Œæ ¼å¼åŒ–æ–‡æœ¬çš„å­—å…¸
    """
    if not settings.BOCHA_API_KEY:
        raise ValueError("BOCHA_API_KEY is not set in environment variables")

    url = 'https://api.bochaai.com/v1/web-search'
    headers = {
        'Authorization': f'Bearer {settings.BOCHA_API_KEY}',
        'Content-Type': 'application/json'
    }
    data = {
        "query": query,
        "freshness": "noLimit",  # æœç´¢çš„æ—¶é—´èŒƒå›´
        "summary": True,  # æ˜¯å¦è¿”å›é•¿æ–‡æœ¬æ‘˜è¦
        "count": count
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            json_response = response.json()
            if json_response.get("code") != 200 or not json_response.get("data"):
                error_msg = json_response.get("msg", "æœªçŸ¥é”™è¯¯")
                logger.error(f"åšæŸ¥æœç´¢APIè¯·æ±‚å¤±è´¥: {error_msg}")
                return {
                    "webpages": [],
                    "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒåŸå› æ˜¯: {error_msg}"
                }
            
            webpages = json_response.get("data", {}).get("webPages", {}).get("value", [])
            if not webpages:
                logger.warning(f"åšæŸ¥æœç´¢APIè¿”å›ç©ºç»“æœï¼ŒæŸ¥è¯¢: {query[:100]}...")
                return {
                    "webpages": [],
                    "formatted_text": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
                }
            
            logger.info(f"åšæŸ¥æœç´¢APIæˆåŠŸè¿”å› {len(webpages)} ä¸ªç»“æœï¼ŒæŸ¥è¯¢: {query[:100]}...")
            formatted_text = format_bocha_search_results(webpages)
            return {
                "webpages": webpages,
                "formatted_text": formatted_text
            }
        else:
            error_msg = f"çŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {response.text}"
            logger.error(f"åšæŸ¥æœç´¢APIè¯·æ±‚å¤±è´¥: {error_msg}")
            return {
                "webpages": [],
                "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼Œ{error_msg}"
            }
    except Exception as e:
        logger.error(f"åšæŸ¥æœç´¢APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return {
            "webpages": [],
            "formatted_text": f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒåŸå› æ˜¯ï¼š{str(e)}"
        }


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """
    ä½¿ç”¨åšæŸ¥æœç´¢ API è¿›è¡Œç½‘é¡µç ”ç©¶ã€‚
    """
    search_query = state["search_query"]
    search_id = state["id"]
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡ ID={search_id}")
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢æŸ¥è¯¢: {search_query[:200]}...")
    
    # è°ƒç”¨åšæŸ¥æœç´¢ API
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘è°ƒç”¨åšæŸ¥æœç´¢ API...")
    search_result = bocha_web_search(query=search_query, count=10)
    
    webpages = search_result.get("webpages", [])
    formatted_text = search_result.get("formatted_text", "")
    
    webpage_count = len(webpages) if webpages else 0
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {webpage_count} ä¸ªç½‘é¡µç»“æœ")
    
    if webpage_count > 0:
        logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å‰3ä¸ªç»“æœæ ‡é¢˜:")
        for idx, page in enumerate(webpages[:3], 1):
            logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘  {idx}. {page.get('name', 'N/A')[:100]}")
    
    # ä½¿ç”¨ Gemini å¯¹æœç´¢ç»“æœè¿›è¡Œæ€»ç»“å’Œæ•´ç†
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹ä½¿ç”¨ LLM æ€»ç»“æœç´¢ç»“æœ...")
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=search_query,
    )

    # å°†æœç´¢ç»“æœæ·»åŠ åˆ°æç¤ºè¯ä¸­ï¼Œå¹¶æç¤º LLM ä½¿ç”¨å¼•ç”¨ç¼–å·
    search_context = (
        f"\n\næœç´¢æŸ¥è¯¢: {search_query}\n"
        f"æœç´¢ç»“æœï¼ˆè¯·åœ¨ä½ çš„å›ç­”ä¸­ä½¿ç”¨å¼•ç”¨ç¼–å· [1], [2] ç­‰æ¥å¼•ç”¨è¿™äº›æ¥æºï¼‰:\n{formatted_text}"
    )
    full_prompt = formatted_prompt + search_context

    gemini_base_url = get_gemini_base_url()
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘Gemini API URL: {gemini_base_url}")
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘Gemini Model: {settings.GEMINI_MODEL}")
    
    llm = ChatOpenAI(
        model=settings.GEMINI_MODEL,
        temperature=0,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘è°ƒç”¨ Gemini APIï¼Œæç¤ºè¯é•¿åº¦: {len(full_prompt)} å­—ç¬¦")
    llm_response = llm.invoke(full_prompt)
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘LLM æ€»ç»“å®Œæˆï¼Œå“åº”é•¿åº¦: {len(llm_response.content)} å­—ç¬¦")
    
    # å¤„ç†å¼•ç”¨å’Œæ¥æº
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¼€å§‹å¤„ç†å¼•ç”¨å’Œæ¥æº...")
    resolved_urls = resolve_urls(webpages, search_id)
    citations = get_citations_from_bocha(webpages, resolved_urls, llm_response.content)
    modified_text = insert_citation_markers(llm_response.content, citations)
    sources_gathered = [item for citation in citations for item in citation["segments"]]
    
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(citations)} ä¸ªå¼•ç”¨ï¼Œ{len(sources_gathered)} ä¸ªæ•°æ®æºç‰‡æ®µ")
    logger.info(f"ã€èŠ‚ç‚¹: web_researchã€‘æœç´¢ä»»åŠ¡ ID={search_id} å®Œæˆ")

    return {
        "sources_gathered": sources_gathered,
        "search_query": [search_query],
        "web_research_result": [modified_text],
    }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    loop_count = state["research_loop_count"]
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘å¼€å§‹åæ€ï¼Œç ”ç©¶å¾ªç¯æ¬¡æ•°: {loop_count}")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    web_research_results = state.get("web_research_result", [])
    search_queries = state.get("search_query", [])
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘å½“å‰å·²æœ‰ {len(web_research_results)} ä¸ªæœç´¢ç»“æœï¼Œ{len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")

    formatted_prompt = reflection_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        loop_count=loop_count,
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    
    logger.info("ã€èŠ‚ç‚¹: reflectionã€‘è°ƒç”¨ LLM è¿›è¡Œåæ€è¯„ä¼°...")
    gemini_base_url = get_gemini_base_url()
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘Gemini API URL: {gemini_base_url}")
    
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=1.0,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)

    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘=== ä¿¡æ¯å……è¶³æ€§è¯„ä¼°ç»“æœ ===")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  å½“å‰å¾ªç¯: ç¬¬ {loop_count} è½®")
    logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  ä¿¡æ¯æ˜¯å¦å……è¶³: {result.is_sufficient}")
    
    if result.is_sufficient:
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  âœ… è¯„ä¼°ç»“æœ: ä¿¡æ¯å·²å……è¶³ï¼Œå¯ä»¥å¼€å§‹ç”ŸæˆæŠ¥å‘Š")
    else:
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  âš ï¸  è¯„ä¼°ç»“æœ: ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç»§ç»­ç ”ç©¶")
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  çŸ¥è¯†ç¼ºå£: {result.knowledge_gap[:200] if result.knowledge_gap else 'N/A'}...")
        follow_up_count = len(result.follow_up_queries) if result.follow_up_queries else 0
        logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘  åç»­æŸ¥è¯¢æ•°é‡: {follow_up_count}")
        if follow_up_count > 0:
            for idx, query in enumerate(result.follow_up_queries[:3], 1):  # åªè®°å½•å‰3ä¸ª
                logger.info(f"ã€èŠ‚ç‚¹: reflectionã€‘    åç»­æŸ¥è¯¢ {idx}: {query[:100]}...")

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
        "max_research_loops": state.get("max_research_loops", 5),  # ä¼ é€’æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œé»˜è®¤5
    }


def evaluate_research(state: ReflectionState, config: RunnableConfig) -> OverallState:
    max_research_loops = state.get("max_research_loops", 5)  # é»˜è®¤5æ¬¡å¾ªç¯
    loop_count = state["research_loop_count"]
    is_sufficient = state["is_sufficient"]
    
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘=== ç ”ç©¶çŠ¶æ€å†³ç­– ===")
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘  å½“å‰å¾ªç¯æ¬¡æ•°: {loop_count}/{max_research_loops}")
    logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘  ä¿¡æ¯æ˜¯å¦å……è¶³: {is_sufficient}")
    
    if state["is_sufficient"]:
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘âœ… å†³ç­–: ä¿¡æ¯å·²å……è¶³ï¼Œç»“æŸè°ƒæŸ¥å¾ªç¯")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: å¼€å§‹è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹")
        return "assess_content_quality"
    elif state["research_loop_count"] >= max_research_loops:
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘âš ï¸  å†³ç­–: å·²è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° ({max_research_loops})ï¼Œå¼ºåˆ¶ç»“æŸ")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: åŸºäºç°æœ‰ä¿¡æ¯ç”ŸæˆæŠ¥å‘Š")
        return "assess_content_quality"
    else:
        follow_up_queries = state.get("follow_up_queries", [])
        follow_up_count = len(follow_up_queries)
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘ğŸ”„ å†³ç­–: ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­ç¬¬ {loop_count + 1} è½®è°ƒæŸ¥")
        logger.info(f"ã€èŠ‚ç‚¹: evaluate_researchã€‘â¡ï¸  ä¸‹ä¸€æ­¥: åˆ†å‘ {follow_up_count} ä¸ªåç»­æŸ¥è¯¢åˆ° web_research èŠ‚ç‚¹")
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def assess_content_quality(state: OverallState, config: RunnableConfig):
    """å†…å®¹è´¨é‡è¯„ä¼°èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘å¼€å§‹å†…å®¹è´¨é‡è¯„ä¼°")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    formatted_prompt = content_quality_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    gemini_base_url = get_gemini_base_url()
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0.3,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    
    result = llm.with_structured_output(ContentQualityAssessment).invoke(formatted_prompt)
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘è´¨é‡è¯„åˆ†: {result.quality_score}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_content_qualityã€‘å†…å®¹ç©ºç™½æ•°é‡: {len(result.content_gaps)}")
    
    return {
        "content_quality": {
            "quality_score": result.quality_score,
            "reliability_assessment": result.reliability_assessment,
            "content_gaps": result.content_gaps,
            "improvement_suggestions": result.improvement_suggestions
        }
    }


def verify_facts(state: OverallState, config: RunnableConfig):
    """äº‹å®éªŒè¯èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘å¼€å§‹äº‹å®éªŒè¯")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = fact_verification_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    gemini_base_url = get_gemini_base_url()
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0.1,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    
    # ä½¿ç”¨ include_raw=False å’Œ method å‚æ•°ç¡®ä¿ Gemini å…¼å®¹
    result = llm.with_structured_output(
        FactVerification,
        method="json_schema",
        include_raw=False
    ).invoke(formatted_prompt)
    
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘éªŒè¯ç½®ä¿¡åº¦: {result.confidence_score}")
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘å·²éªŒè¯äº‹å®æ•°é‡: {len(result.verified_facts_text)}")
    logger.info(f"ã€èŠ‚ç‚¹: verify_factsã€‘äº‰è®®å£°æ˜æ•°é‡: {len(result.disputed_claims_text)}")
    
    # å°†æ‰å¹³åŒ–çš„åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    verified_facts_dicts = [
        {"fact": fact, "source": source} 
        for fact, source in zip(result.verified_facts_text, result.verified_facts_sources)
    ]
    disputed_claims_dicts = [
        {"claim": claim, "reason": reason} 
        for claim, reason in zip(result.disputed_claims_text, result.disputed_claims_reasons)
    ]
    
    return {
        "fact_verification": {
            "verified_facts": verified_facts_dicts,
            "disputed_claims": disputed_claims_dicts,
            "verification_sources": result.verification_sources,
            "confidence_score": result.confidence_score
        }
    }


def assess_relevance(state: OverallState, config: RunnableConfig):
    """ç›¸å…³æ€§è¯„ä¼°èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘å¼€å§‹ç›¸å…³æ€§è¯„ä¼°")
    
    # åˆå¹¶æ‰€æœ‰ç ”ç©¶å†…å®¹
    combined_content = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    formatted_prompt = relevance_assessment_instructions.format(
        research_topic=get_research_topic(state["messages"]),
        content=combined_content
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    gemini_base_url = get_gemini_base_url()
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0.2,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    
    result = llm.with_structured_output(RelevanceAssessment).invoke(formatted_prompt)
    
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ç›¸å…³æ€§è¯„åˆ†: {result.relevance_score}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘è¦†ç›–å…³é”®ä¸»é¢˜æ•°é‡: {len(result.key_topics_covered)}")
    logger.info(f"ã€èŠ‚ç‚¹: assess_relevanceã€‘ç¼ºå¤±ä¸»é¢˜æ•°é‡: {len(result.missing_topics)}")
    
    return {
        "relevance_assessment": {
            "relevance_score": result.relevance_score,
            "key_topics_covered": result.key_topics_covered,
            "missing_topics": result.missing_topics,
            "content_alignment": result.content_alignment
        }
    }


def optimize_summary(state: OverallState, config: RunnableConfig):
    """æ‘˜è¦ä¼˜åŒ–èŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å¼€å§‹æ‘˜è¦ä¼˜åŒ–")
    
    # è·å–åŸå§‹æ‘˜è¦
    original_summary = "\n\n---\n\n".join(state.get("web_research_result", []))
    
    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = summary_optimization_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        original_summary=original_summary,
        quality_assessment=str(state.get("content_quality", {})),
        fact_verification=str(state.get("fact_verification", {})),
        relevance_assessment=str(state.get("relevance_assessment", {}))
    )
    
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    gemini_base_url = get_gemini_base_url()
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0.3,
        max_retries=2,
        api_key=settings.GEMINI_API_KEY,
        base_url=gemini_base_url,
        timeout=settings.API_TIMEOUT,
    )
    
    result = llm.with_structured_output(SummaryOptimization).invoke(formatted_prompt)
    
    # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†
    quality_score = state.get("content_quality", {}).get("quality_score", 0.5)
    fact_confidence = state.get("fact_verification", {}).get("confidence_score", 0.5)
    relevance_score = state.get("relevance_assessment", {}).get("relevance_score", 0.5)
    final_confidence = (quality_score + fact_confidence + relevance_score) / 3
    
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘ä¼˜åŒ–æ‘˜è¦é•¿åº¦: {len(result.optimized_summary)} å­—ç¬¦")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å…³é”®æ´å¯Ÿæ•°é‡: {len(result.key_insights)}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘å¯è¡Œå»ºè®®æ•°é‡: {len(result.actionable_items)}")
    logger.info(f"ã€èŠ‚ç‚¹: optimize_summaryã€‘æœ€ç»ˆç½®ä¿¡åº¦: {final_confidence:.3f}")
    
    return {
        "summary_optimization": {
            "optimized_summary": result.optimized_summary,
            "key_insights": result.key_insights,
            "actionable_items": result.actionable_items,
            "confidence_level": result.confidence_level
        },
        "quality_enhanced_summary": result.optimized_summary,
        "final_confidence_score": final_confidence
    }


def generate_verification_report(state: OverallState, config: RunnableConfig):
    """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘ŠèŠ‚ç‚¹ã€‚"""
    logger.info(f"ã€èŠ‚ç‚¹: generate_verification_reportã€‘å¼€å§‹ç”ŸæˆéªŒè¯æŠ¥å‘Š")
    
    # ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š
    quality_data = state.get("content_quality", {})
    fact_data = state.get("fact_verification", {})
    relevance_data = state.get("relevance_assessment", {})
    optimization_data = state.get("summary_optimization", {})
    
    report = f"""
# ç ”ç©¶è´¨é‡éªŒè¯æŠ¥å‘Š

## å†…å®¹è´¨é‡è¯„ä¼°
- **è´¨é‡è¯„åˆ†**: {quality_data.get('quality_score', 'N/A'):.2f}/1.0
- **å¯é æ€§è¯„ä¼°**: {quality_data.get('reliability_assessment', 'N/A')}
- **å†…å®¹ç©ºç™½**: {', '.join(quality_data.get('content_gaps', [])) if quality_data.get('content_gaps') else 'æ— æ˜æ˜¾ç©ºç™½'}
- **æ”¹è¿›å»ºè®®**: {', '.join(quality_data.get('improvement_suggestions', [])) if quality_data.get('improvement_suggestions') else 'æ— ç‰¹åˆ«å»ºè®®'}

## äº‹å®éªŒè¯ç»“æœ
- **éªŒè¯ç½®ä¿¡åº¦**: {fact_data.get('confidence_score', 'N/A'):.2f}/1.0
- **å·²éªŒè¯äº‹å®æ•°é‡**: {len(fact_data.get('verified_facts', []))}
- **äº‰è®®å£°æ˜æ•°é‡**: {len(fact_data.get('disputed_claims', []))}
- **éªŒè¯æ¥æº**: {', '.join(fact_data.get('verification_sources', [])) if fact_data.get('verification_sources') else 'å¤šä¸ªæ¥æº'}

## ç›¸å…³æ€§è¯„ä¼°
- **ç›¸å…³æ€§è¯„åˆ†**: {relevance_data.get('relevance_score', 'N/A'):.2f}/1.0
- **å·²è¦†ç›–å…³é”®ä¸»é¢˜**: {', '.join(relevance_data.get('key_topics_covered', [])) if relevance_data.get('key_topics_covered') else 'N/A'}
- **ç¼ºå¤±ä¸»é¢˜**: {', '.join(relevance_data.get('missing_topics', [])) if relevance_data.get('missing_topics') else 'æ— æ˜æ˜¾ç¼ºå¤±'}
- **å†…å®¹ä¸€è‡´æ€§**: {relevance_data.get('content_alignment', 'N/A')}

## æ‘˜è¦ä¼˜åŒ–ç»“æœ
- **ç½®ä¿¡åº¦ç­‰çº§**: {optimization_data.get('confidence_level', 'N/A')}
- **å…³é”®æ´å¯Ÿæ•°é‡**: {len(optimization_data.get('key_insights', []))}
- **å¯è¡Œå»ºè®®æ•°é‡**: {len(optimization_data.get('actionable_items', []))}

## ç»¼åˆè¯„ä¼°
- **æœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†**: {state.get('final_confidence_score', 0):.3f}/1.0
"""
    
    logger.info(f"ã€èŠ‚ç‚¹: generate_verification_reportã€‘éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    return {
        "verification_report": report
    }


def finalize_answer(state: OverallState, config: RunnableConfig):
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œè¿”å›é«˜åº¦å›´ç»•ç”¨æˆ·æé—®çš„è°ƒæŸ¥ç ”ç©¶æŠ¥å‘Šã€‚"""
    reasoning_model = state.get("reasoning_model") or settings.GEMINI_MODEL
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘å¼€å§‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
    
    web_research_results = state.get("web_research_result", [])
    sources_gathered = state.get("sources_gathered", [])
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æ±‡æ€» {len(web_research_results)} ä¸ªæœç´¢ç»“æœï¼Œ{len(sources_gathered)} ä¸ªæ•°æ®æº")

    # ä½¿ç”¨ä¼˜åŒ–åçš„æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹æ‘˜è¦
    final_summary = state.get("quality_enhanced_summary")
    if not final_summary:
        logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘ä½¿ç”¨åŸå§‹æ‘˜è¦")
        formatted_prompt = answer_instructions.format(
            current_date=get_current_date(),
            research_topic=get_research_topic(state["messages"]),
            summaries="\n---\n\n".join(state["web_research_result"]),
        )
        
        logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
        gemini_base_url = get_gemini_base_url()
        
        llm = ChatOpenAI(
            model=reasoning_model,
            temperature=0,
            max_retries=2,
            api_key=settings.GEMINI_API_KEY,
            base_url=gemini_base_url,
            timeout=settings.API_TIMEOUT,
        )
        result = llm.invoke(formatted_prompt)
        final_summary = result.content
        logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘LLM ç”Ÿæˆå®Œæˆï¼Œç­”æ¡ˆé•¿åº¦: {len(final_summary)} å­—ç¬¦")
    else:
        logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘ä½¿ç”¨è´¨é‡å¢å¼ºçš„ä¼˜åŒ–æ‘˜è¦")

    # åªè¿”å›ä¼˜åŒ–åçš„è°ƒæŸ¥ç ”ç©¶æŠ¥å‘Šï¼Œä¸åŒ…å«éªŒè¯æŠ¥å‘Šå’Œè´¨é‡æŒ‡æ ‡
    enhanced_content = final_summary
    
    logger.info("ã€èŠ‚ç‚¹: finalize_answerã€‘å¤„ç†æ•°æ®æºå¼•ç”¨...")
    unique_sources: List[Dict[str, Any]] = []
    for source in sources_gathered:
        if source["short_url"] in enhanced_content:
            enhanced_content = enhanced_content.replace(source["short_url"], source["value"])
            unique_sources.append(source)
    
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æœ€ç»ˆç­”æ¡ˆåŒ…å« {len(unique_sources)} ä¸ªè¢«å¼•ç”¨çš„æ•°æ®æº")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘æœ€ç»ˆå†…å®¹é•¿åº¦: {len(enhanced_content)} å­—ç¬¦")
    logger.info(f"ã€èŠ‚ç‚¹: finalize_answerã€‘èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")

    return {
        "messages": [AIMessage(content=enhanced_content)],
        "sources_gathered": unique_sources,
    }


_builder = StateGraph(OverallState)
_builder.add_node("generate_query", generate_query)
_builder.add_node("web_research", web_research)
_builder.add_node("reflection", reflection)
# æ·»åŠ è´¨é‡å¢å¼ºèŠ‚ç‚¹
_builder.add_node("assess_content_quality", assess_content_quality)
_builder.add_node("verify_facts", verify_facts)
_builder.add_node("assess_relevance", assess_relevance)
_builder.add_node("optimize_summary", optimize_summary)
_builder.add_node("generate_verification_report", generate_verification_report)
_builder.add_node("finalize_answer", finalize_answer)

# è®¾ç½®å…¥å£ç‚¹
_builder.add_edge(START, "generate_query")
_builder.add_conditional_edges("generate_query", continue_to_web_research, ["web_research"])
_builder.add_edge("web_research", "reflection")
_builder.add_conditional_edges("reflection", evaluate_research, ["web_research", "assess_content_quality"])

# è´¨é‡å¢å¼ºæµç¨‹
_builder.add_edge("assess_content_quality", "verify_facts")
_builder.add_edge("verify_facts", "assess_relevance")
_builder.add_edge("assess_relevance", "optimize_summary")
_builder.add_edge("optimize_summary", "generate_verification_report")
_builder.add_edge("generate_verification_report", "finalize_answer")

# ç»“æŸèŠ‚ç‚¹
_builder.add_edge("finalize_answer", END)

graph = _builder.compile(name="enhanced-pro-search-engine")

logger.info("ã€å›¾æ„å»ºå®Œæˆã€‘å¢å¼ºå‹ Pro Search Engine å·²ç¼–è¯‘å®Œæˆ")


